{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__verSN:1__<br>\n",
    "__Image__<br>\n",
    "metaval:resizeのみ<br>\n",
    "192_192：OOF: 0.869 Wall time:  1h 33min LB: 0.861<br>\n",
    "224_224：OOF: 0.883 Wall time:  2h 07min LB: 0.862<br>\n",
    "256_256：OOF: 0.875 Wall time:  2h 36min LB: 0.882<br>\n",
    "382_382：OOF: 0.869 Wall time:  5h 10min LB: 0.854<br>\n",
    "512_512：OOF: 0.848 Wall time: 10h 29min LB: 0.852<br>\n",
    "__Table__<br>\n",
    "patientid有、isnan有<br>\n",
    "lgb: CV mean score: 0.7306, std: 0.0175., LB: 0.6637<br>\n",
    "SVC: CV mean score: 0.6622, std: 0.0276., LB: 0.6637<br>\n",
    "__Blending__<br>\n",
    "blending model<br>\n",
    "lgb: CV mean score: 0.8838, std: 0.0172., LB: 0.8642<br>\n",
    "SVC: CV mean score: 0.8103, std: 0.0634., LB: 0.8342<br>\n",
    "BayesianOptimization<br>\n",
    "init_points = 50, n_iter = 10, CV:0.8981, LB:0.8811<br>\n",
    "init_points = 50, n_iter = 40, CV:0.8981, LB:0.8989<br>\n",
    "init_points = 100, n_iter = 30, CV:0.8983, LB:0.8813<br>\n",
    "__verSN:3__<br>\n",
    "__Stacking__<br>\n",
    "Stacking model<br>\n",
    "第一層最高スコア：\n",
    "192_192：OOF: 0.893 Wall time: 2h 10min 17s LB: 0.8813<br>\n",
    "BayesianOptimization<br>\n",
    "init_points = 100, n_iter = 30, CV:0.9160, LB:0.8874<br>\n",
    "init_points = 50, n_iter = 40, CV:0.9159, LB:0.8875<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__initialize__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import os\n",
    "from  bayes_opt  import  BayesianOptimization\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "melanoma_external_malignant_256 = \"/home/tidal/ML_Data/SIIM-ISIC_Melanoma_Classification/melanoma_external_malignant_256/\"\n",
    "SIIM_ISIC_Melanoma_Classification = \"/home/tidal/ML_Data/SIIM-ISIC_Melanoma_Classification/SIIM-ISIC-Melanoma-Classification/\"\n",
    "Output = \"/home/tidal/ML_Data/SIIM-ISIC_Melanoma_Classification/Output/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__preprocessing__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#学習用(metaval)/予測用(test)のテーブルデータ作成\n",
    "def generateTable(modelMtx, TrainOrTest):\n",
    "    if TrainOrTest == \"test\":\n",
    "        folder = 'test/sub_test_'\n",
    "        Table = pd.read_csv(SIIM_ISIC_Melanoma_Classification + 'test.csv')\n",
    "        Table = Table[[\"image_name\"]]\n",
    "        \n",
    "    elif TrainOrTest == \"train\":\n",
    "        folder = 'metaval/sub_metaval_'\n",
    "        Table = pd.read_csv(SIIM_ISIC_Melanoma_Classification + 'metaval.csv')\n",
    "        Table = Table[[\"image_name\", \"target\"]]\n",
    "    else:\n",
    "        print(\"err:TrainOrTest is only 'train' or 'test'\")\n",
    "    \n",
    "    for modelNM in modelMtx:\n",
    "        Tb_temp = pd.read_csv(Output + folder + modelNM + '.csv')\n",
    "        Tb_temp = Tb_temp.rename(columns={'target':'pred_' + modelNM})\n",
    "        Table = pd.merge(Table, Tb_temp, on=\"image_name\", how=\"inner\")\n",
    "    \n",
    "    return Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelMtx = ['stacking_64_64', 'stacking_128_128', 'stacking_192_192', 'stacking_224_224', 'stacking_256_256', 'stacking_384_384']\n",
    "MVTable = generateTable(modelMtx = modelMtx, TrainOrTest = \"train\")\n",
    "TestTable = generateTable(modelMtx = modelMtx, TrainOrTest = \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>target</th>\n",
       "      <th>pred_stacking_64_64</th>\n",
       "      <th>pred_stacking_128_128</th>\n",
       "      <th>pred_stacking_192_192</th>\n",
       "      <th>pred_stacking_224_224</th>\n",
       "      <th>pred_stacking_256_256</th>\n",
       "      <th>pred_stacking_384_384</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISIC_0452656</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001004</td>\n",
       "      <td>0.007336</td>\n",
       "      <td>0.005307</td>\n",
       "      <td>0.003105</td>\n",
       "      <td>0.001621</td>\n",
       "      <td>0.000569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISIC_0507189</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001738</td>\n",
       "      <td>0.003728</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>0.000150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ISIC_1144286</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005044</td>\n",
       "      <td>0.003673</td>\n",
       "      <td>0.009989</td>\n",
       "      <td>0.012324</td>\n",
       "      <td>0.041376</td>\n",
       "      <td>0.018509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ISIC_1199236</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000980</td>\n",
       "      <td>0.001144</td>\n",
       "      <td>0.006216</td>\n",
       "      <td>0.002695</td>\n",
       "      <td>0.003616</td>\n",
       "      <td>0.003348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ISIC_1303205</td>\n",
       "      <td>0</td>\n",
       "      <td>0.050943</td>\n",
       "      <td>0.018445</td>\n",
       "      <td>0.025705</td>\n",
       "      <td>0.004842</td>\n",
       "      <td>0.030912</td>\n",
       "      <td>0.009495</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     image_name  target  pred_stacking_64_64  pred_stacking_128_128  \\\n",
       "0  ISIC_0452656       0             0.001004               0.007336   \n",
       "1  ISIC_0507189       0             0.001738               0.003728   \n",
       "2  ISIC_1144286       0             0.005044               0.003673   \n",
       "3  ISIC_1199236       0             0.000980               0.001144   \n",
       "4  ISIC_1303205       0             0.050943               0.018445   \n",
       "\n",
       "   pred_stacking_192_192  pred_stacking_224_224  pred_stacking_256_256  \\\n",
       "0               0.005307               0.003105               0.001621   \n",
       "1               0.000093               0.000093               0.000369   \n",
       "2               0.009989               0.012324               0.041376   \n",
       "3               0.006216               0.002695               0.003616   \n",
       "4               0.025705               0.004842               0.030912   \n",
       "\n",
       "   pred_stacking_384_384  \n",
       "0               0.000569  \n",
       "1               0.000150  \n",
       "2               0.018509  \n",
       "3               0.003348  \n",
       "4               0.009495  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MVTable.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>target</th>\n",
       "      <th>pred_stacking_64_64</th>\n",
       "      <th>pred_stacking_128_128</th>\n",
       "      <th>pred_stacking_192_192</th>\n",
       "      <th>pred_stacking_224_224</th>\n",
       "      <th>pred_stacking_256_256</th>\n",
       "      <th>pred_stacking_384_384</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ISIC_5147292</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.056768</td>\n",
       "      <td>0.001169</td>\n",
       "      <td>0.006450</td>\n",
       "      <td>0.013042</td>\n",
       "      <td>0.000922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>ISIC_2837581</td>\n",
       "      <td>1</td>\n",
       "      <td>0.110931</td>\n",
       "      <td>0.036893</td>\n",
       "      <td>0.007663</td>\n",
       "      <td>0.007940</td>\n",
       "      <td>0.067592</td>\n",
       "      <td>0.055874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>ISIC_9583969</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005159</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.022254</td>\n",
       "      <td>0.008172</td>\n",
       "      <td>0.033650</td>\n",
       "      <td>0.030052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>ISIC_9228573</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002107</td>\n",
       "      <td>0.101218</td>\n",
       "      <td>0.030195</td>\n",
       "      <td>0.003336</td>\n",
       "      <td>0.065176</td>\n",
       "      <td>0.010392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>ISIC_4425322</td>\n",
       "      <td>1</td>\n",
       "      <td>0.062661</td>\n",
       "      <td>0.056434</td>\n",
       "      <td>0.011216</td>\n",
       "      <td>0.081924</td>\n",
       "      <td>0.027016</td>\n",
       "      <td>0.027704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7988</th>\n",
       "      <td>ISIC_6348638</td>\n",
       "      <td>1</td>\n",
       "      <td>0.041607</td>\n",
       "      <td>0.015369</td>\n",
       "      <td>0.107629</td>\n",
       "      <td>0.046254</td>\n",
       "      <td>0.038035</td>\n",
       "      <td>0.041245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8231</th>\n",
       "      <td>ISIC_4517738</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002506</td>\n",
       "      <td>0.019200</td>\n",
       "      <td>0.022240</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.018812</td>\n",
       "      <td>0.003847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8285</th>\n",
       "      <td>ISIC_2105563</td>\n",
       "      <td>1</td>\n",
       "      <td>0.008428</td>\n",
       "      <td>0.002705</td>\n",
       "      <td>0.005608</td>\n",
       "      <td>0.008923</td>\n",
       "      <td>0.003404</td>\n",
       "      <td>0.009247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8296</th>\n",
       "      <td>ISIC_5140952</td>\n",
       "      <td>1</td>\n",
       "      <td>0.020964</td>\n",
       "      <td>0.018109</td>\n",
       "      <td>0.011099</td>\n",
       "      <td>0.006078</td>\n",
       "      <td>0.055267</td>\n",
       "      <td>0.007422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8310</th>\n",
       "      <td>ISIC_8609756</td>\n",
       "      <td>1</td>\n",
       "      <td>0.026435</td>\n",
       "      <td>0.014220</td>\n",
       "      <td>0.016818</td>\n",
       "      <td>0.012381</td>\n",
       "      <td>0.047422</td>\n",
       "      <td>0.024421</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>145 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        image_name  target  pred_stacking_64_64  pred_stacking_128_128  \\\n",
       "22    ISIC_5147292       1             0.001541               0.056768   \n",
       "107   ISIC_2837581       1             0.110931               0.036893   \n",
       "120   ISIC_9583969       1             0.005159               0.035714   \n",
       "159   ISIC_9228573       1             0.002107               0.101218   \n",
       "232   ISIC_4425322       1             0.062661               0.056434   \n",
       "...            ...     ...                  ...                    ...   \n",
       "7988  ISIC_6348638       1             0.041607               0.015369   \n",
       "8231  ISIC_4517738       1             0.002506               0.019200   \n",
       "8285  ISIC_2105563       1             0.008428               0.002705   \n",
       "8296  ISIC_5140952       1             0.020964               0.018109   \n",
       "8310  ISIC_8609756       1             0.026435               0.014220   \n",
       "\n",
       "      pred_stacking_192_192  pred_stacking_224_224  pred_stacking_256_256  \\\n",
       "22                 0.001169               0.006450               0.013042   \n",
       "107                0.007663               0.007940               0.067592   \n",
       "120                0.022254               0.008172               0.033650   \n",
       "159                0.030195               0.003336               0.065176   \n",
       "232                0.011216               0.081924               0.027016   \n",
       "...                     ...                    ...                    ...   \n",
       "7988               0.107629               0.046254               0.038035   \n",
       "8231               0.022240               0.002119               0.018812   \n",
       "8285               0.005608               0.008923               0.003404   \n",
       "8296               0.011099               0.006078               0.055267   \n",
       "8310               0.016818               0.012381               0.047422   \n",
       "\n",
       "      pred_stacking_384_384  \n",
       "22                 0.000922  \n",
       "107                0.055874  \n",
       "120                0.030052  \n",
       "159                0.010392  \n",
       "232                0.027704  \n",
       "...                     ...  \n",
       "7988               0.041245  \n",
       "8231               0.003847  \n",
       "8285               0.009247  \n",
       "8296               0.007422  \n",
       "8310               0.024421  \n",
       "\n",
       "[145 rows x 8 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MVTable[MVTable.target == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>pred_stacking_64_64</th>\n",
       "      <th>pred_stacking_128_128</th>\n",
       "      <th>pred_stacking_192_192</th>\n",
       "      <th>pred_stacking_224_224</th>\n",
       "      <th>pred_stacking_256_256</th>\n",
       "      <th>pred_stacking_384_384</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISIC_0052060</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.000809</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000690</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.001446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISIC_0052349</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ISIC_0058510</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ISIC_0073313</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ISIC_0073502</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>0.000273</td>\n",
       "      <td>0.006141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10977</th>\n",
       "      <td>ISIC_9992485</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10978</th>\n",
       "      <td>ISIC_9996992</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10979</th>\n",
       "      <td>ISIC_9997917</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10980</th>\n",
       "      <td>ISIC_9998234</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10981</th>\n",
       "      <td>ISIC_9999302</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10982 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         image_name  pred_stacking_64_64  pred_stacking_128_128  \\\n",
       "0      ISIC_0052060             0.000269               0.000809   \n",
       "1      ISIC_0052349             0.000118               0.000025   \n",
       "2      ISIC_0058510             0.000028               0.000002   \n",
       "3      ISIC_0073313             0.000063               0.000003   \n",
       "4      ISIC_0073502             0.000227               0.000032   \n",
       "...             ...                  ...                    ...   \n",
       "10977  ISIC_9992485             0.000000               0.000000   \n",
       "10978  ISIC_9996992             0.000000               0.000000   \n",
       "10979  ISIC_9997917             0.000000               0.000000   \n",
       "10980  ISIC_9998234             0.000000               0.000000   \n",
       "10981  ISIC_9999302             0.000000               0.000000   \n",
       "\n",
       "       pred_stacking_192_192  pred_stacking_224_224  pred_stacking_256_256  \\\n",
       "0                   0.000263               0.000690               0.000016   \n",
       "1                   0.000004               0.000020               0.000022   \n",
       "2                   0.000023               0.000040               0.000060   \n",
       "3                   0.000006               0.000017               0.000017   \n",
       "4                   0.000123               0.000393               0.000273   \n",
       "...                      ...                    ...                    ...   \n",
       "10977               0.000000               0.000000               0.000000   \n",
       "10978               0.000000               0.000000               0.000000   \n",
       "10979               0.000000               0.000000               0.000000   \n",
       "10980               0.000000               0.000000               0.000000   \n",
       "10981               0.000000               0.000000               0.000000   \n",
       "\n",
       "       pred_stacking_384_384  \n",
       "0                   0.001446  \n",
       "1                   0.000058  \n",
       "2                   0.000033  \n",
       "3                   0.000010  \n",
       "4                   0.006141  \n",
       "...                      ...  \n",
       "10977               0.000000  \n",
       "10978               0.000000  \n",
       "10979               0.000000  \n",
       "10980               0.000000  \n",
       "10981               0.000000  \n",
       "\n",
       "[10982 rows x 7 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TestTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bayes test\n",
    "#parameter(重み)の範囲を定義。\n",
    "params = {}\n",
    "for c in MVTable.columns.drop(['target','image_name']):\n",
    "    params[c] = (0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pred_stacking_64_64': (0, 1),\n",
       " 'pred_stacking_128_128': (0, 1),\n",
       " 'pred_stacking_192_192': (0, 1),\n",
       " 'pred_stacking_224_224': (0, 1),\n",
       " 'pred_stacking_256_256': (0, 1),\n",
       " 'pred_stacking_384_384': (0, 1)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#最大化させたい関数\n",
    "def ROC_evaluate(**params):\n",
    "    warnings.simplefilter('ignore')\n",
    "    \n",
    "    s = sum(params.values())\n",
    "    for p in params:\n",
    "        params[p] = params[p] / s\n",
    "    \n",
    "    test_pred_proba = pd.Series(np.zeros(MVTable.shape[0]), index = MVTable.index)\n",
    "    \n",
    "    feats = [f for f in MVTable.columns if f not in ['target','image_name', 'index']]\n",
    "    \n",
    "    for f in feats:\n",
    "        test_pred_proba += MVTable[f] * params[f]\n",
    "    \n",
    "    return roc_auc_score(MVTable['target'], test_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitBayesian(MVTable, TestTable, init_points, n_iter):\n",
    "    params = {}\n",
    "    for c in MVTable.columns.drop(['target','image_name']):\n",
    "        params[c] = (0, 1)\n",
    "        \n",
    "    bo = BayesianOptimization(ROC_evaluate, params)\n",
    "    #init_points:random explorationのステップ数。exploration space(探索空間)に多様性をもたらす。\n",
    "    #n_iter:bayesian optimizationのステップ数\n",
    "    bo.maximize(init_points = init_points, n_iter = n_iter)\n",
    "    best_params = bo.max['params']\n",
    "    best_normalized_params = {}\n",
    "    \n",
    "    s = sum(best_params.values())\n",
    "    for p in best_params:\n",
    "        best_normalized_params[p] = best_params[p] / s\n",
    "\n",
    "    prediction_train = pd.Series(np.zeros(MVTable.shape[0]), index = MVTable.index)\n",
    "    prediction_test = pd.Series(np.zeros(TestTable.shape[0]), index = TestTable.index)\n",
    "        \n",
    "    feats = [f for f in MVTable.columns if f not in ['target','image_name', 'index']]\n",
    "        \n",
    "    for f in feats:\n",
    "        prediction_train += MVTable[f] * best_normalized_params[f]\n",
    "        prediction_test += TestTable[f] * best_normalized_params[f]\n",
    "        \n",
    "    train_score = roc_auc_score(MVTable['target'], prediction_train)\n",
    "    \n",
    "    print(\"init_points = {0}, n_iter = {1}, CV:{2:.4f}\".format(init_points, n_iter, train_score))\n",
    "    \n",
    "    return prediction_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | pred_s... | pred_s... | pred_s... | pred_s... | pred_s... | pred_s... |\n",
      "-------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.9132  \u001b[0m | \u001b[0m 0.3546  \u001b[0m | \u001b[0m 0.9538  \u001b[0m | \u001b[0m 0.5884  \u001b[0m | \u001b[0m 0.8679  \u001b[0m | \u001b[0m 0.5521  \u001b[0m | \u001b[0m 0.7607  \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.912   \u001b[0m | \u001b[0m 0.312   \u001b[0m | \u001b[0m 0.6755  \u001b[0m | \u001b[0m 0.2485  \u001b[0m | \u001b[0m 0.5862  \u001b[0m | \u001b[0m 0.6216  \u001b[0m | \u001b[0m 0.9334  \u001b[0m |\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.9141  \u001b[0m | \u001b[95m 0.5024  \u001b[0m | \u001b[95m 0.07519 \u001b[0m | \u001b[95m 0.257   \u001b[0m | \u001b[95m 0.7748  \u001b[0m | \u001b[95m 0.5443  \u001b[0m | \u001b[95m 0.3072  \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.91    \u001b[0m | \u001b[0m 0.176   \u001b[0m | \u001b[0m 0.1271  \u001b[0m | \u001b[0m 0.8536  \u001b[0m | \u001b[0m 0.4346  \u001b[0m | \u001b[0m 0.6822  \u001b[0m | \u001b[0m 0.3777  \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.911   \u001b[0m | \u001b[0m 0.4308  \u001b[0m | \u001b[0m 0.5519  \u001b[0m | \u001b[0m 0.8792  \u001b[0m | \u001b[0m 0.3465  \u001b[0m | \u001b[0m 0.4702  \u001b[0m | \u001b[0m 0.08264 \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.9074  \u001b[0m | \u001b[0m 0.3075  \u001b[0m | \u001b[0m 0.165   \u001b[0m | \u001b[0m 0.7065  \u001b[0m | \u001b[0m 0.1533  \u001b[0m | \u001b[0m 0.5703  \u001b[0m | \u001b[0m 0.9878  \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.9079  \u001b[0m | \u001b[0m 0.663   \u001b[0m | \u001b[0m 0.1637  \u001b[0m | \u001b[0m 0.6363  \u001b[0m | \u001b[0m 0.07425 \u001b[0m | \u001b[0m 0.2676  \u001b[0m | \u001b[0m 0.1669  \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.9121  \u001b[0m | \u001b[0m 0.7122  \u001b[0m | \u001b[0m 0.4681  \u001b[0m | \u001b[0m 0.8214  \u001b[0m | \u001b[0m 0.3695  \u001b[0m | \u001b[0m 0.05086 \u001b[0m | \u001b[0m 0.06607 \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.9072  \u001b[0m | \u001b[0m 0.09289 \u001b[0m | \u001b[0m 0.1761  \u001b[0m | \u001b[0m 0.8361  \u001b[0m | \u001b[0m 0.3134  \u001b[0m | \u001b[0m 0.9096  \u001b[0m | \u001b[0m 0.8021  \u001b[0m |\n",
      "| \u001b[95m 10      \u001b[0m | \u001b[95m 0.9141  \u001b[0m | \u001b[95m 0.549   \u001b[0m | \u001b[95m 0.7349  \u001b[0m | \u001b[95m 0.06138 \u001b[0m | \u001b[95m 0.8278  \u001b[0m | \u001b[95m 0.58    \u001b[0m | \u001b[95m 0.1266  \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.905   \u001b[0m | \u001b[0m 0.1786  \u001b[0m | \u001b[0m 0.08248 \u001b[0m | \u001b[0m 0.6871  \u001b[0m | \u001b[0m 0.07462 \u001b[0m | \u001b[0m 0.6928  \u001b[0m | \u001b[0m 0.4029  \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.9094  \u001b[0m | \u001b[0m 0.9407  \u001b[0m | \u001b[0m 0.9834  \u001b[0m | \u001b[0m 0.5304  \u001b[0m | \u001b[0m 0.1111  \u001b[0m | \u001b[0m 0.1457  \u001b[0m | \u001b[0m 0.7638  \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.9099  \u001b[0m | \u001b[0m 0.06975 \u001b[0m | \u001b[0m 0.471   \u001b[0m | \u001b[0m 0.558   \u001b[0m | \u001b[0m 0.2884  \u001b[0m | \u001b[0m 0.382   \u001b[0m | \u001b[0m 0.5931  \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.9125  \u001b[0m | \u001b[0m 0.4028  \u001b[0m | \u001b[0m 0.1049  \u001b[0m | \u001b[0m 0.4149  \u001b[0m | \u001b[0m 0.7442  \u001b[0m | \u001b[0m 0.877   \u001b[0m | \u001b[0m 0.2556  \u001b[0m |\n",
      "| \u001b[95m 15      \u001b[0m | \u001b[95m 0.9146  \u001b[0m | \u001b[95m 0.6137  \u001b[0m | \u001b[95m 0.928   \u001b[0m | \u001b[95m 0.2912  \u001b[0m | \u001b[95m 0.8097  \u001b[0m | \u001b[95m 0.06348 \u001b[0m | \u001b[95m 0.1564  \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 0.9092  \u001b[0m | \u001b[0m 0.6581  \u001b[0m | \u001b[0m 0.6992  \u001b[0m | \u001b[0m 0.2286  \u001b[0m | \u001b[0m 0.1959  \u001b[0m | \u001b[0m 0.6883  \u001b[0m | \u001b[0m 0.9094  \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 0.9128  \u001b[0m | \u001b[0m 0.6104  \u001b[0m | \u001b[0m 0.4561  \u001b[0m | \u001b[0m 0.7387  \u001b[0m | \u001b[0m 0.5723  \u001b[0m | \u001b[0m 0.3855  \u001b[0m | \u001b[0m 0.654   \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 0.9115  \u001b[0m | \u001b[0m 0.727   \u001b[0m | \u001b[0m 0.7996  \u001b[0m | \u001b[0m 0.08021 \u001b[0m | \u001b[0m 0.2817  \u001b[0m | \u001b[0m 0.3665  \u001b[0m | \u001b[0m 0.2624  \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 0.9119  \u001b[0m | \u001b[0m 0.5412  \u001b[0m | \u001b[0m 0.7245  \u001b[0m | \u001b[0m 0.9425  \u001b[0m | \u001b[0m 0.4978  \u001b[0m | \u001b[0m 0.4953  \u001b[0m | \u001b[0m 0.5062  \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 0.9115  \u001b[0m | \u001b[0m 0.7723  \u001b[0m | \u001b[0m 0.4087  \u001b[0m | \u001b[0m 0.7465  \u001b[0m | \u001b[0m 0.3945  \u001b[0m | \u001b[0m 0.1759  \u001b[0m | \u001b[0m 0.9398  \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m 0.9136  \u001b[0m | \u001b[0m 0.1259  \u001b[0m | \u001b[0m 0.7292  \u001b[0m | \u001b[0m 0.04864 \u001b[0m | \u001b[0m 0.7866  \u001b[0m | \u001b[0m 0.3566  \u001b[0m | \u001b[0m 0.6145  \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m 0.9087  \u001b[0m | \u001b[0m 0.308   \u001b[0m | \u001b[0m 0.5916  \u001b[0m | \u001b[0m 0.7179  \u001b[0m | \u001b[0m 0.1533  \u001b[0m | \u001b[0m 0.4676  \u001b[0m | \u001b[0m 0.9045  \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m 0.9119  \u001b[0m | \u001b[0m 0.2233  \u001b[0m | \u001b[0m 0.1164  \u001b[0m | \u001b[0m 0.8032  \u001b[0m | \u001b[0m 0.8596  \u001b[0m | \u001b[0m 0.7794  \u001b[0m | \u001b[0m 0.9984  \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m 0.9089  \u001b[0m | \u001b[0m 0.9646  \u001b[0m | \u001b[0m 0.2657  \u001b[0m | \u001b[0m 0.5067  \u001b[0m | \u001b[0m 0.146   \u001b[0m | \u001b[0m 0.0366  \u001b[0m | \u001b[0m 0.6459  \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m 0.9131  \u001b[0m | \u001b[0m 0.4282  \u001b[0m | \u001b[0m 0.04746 \u001b[0m | \u001b[0m 0.8127  \u001b[0m | \u001b[0m 0.989   \u001b[0m | \u001b[0m 0.6511  \u001b[0m | \u001b[0m 0.9363  \u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m 0.9048  \u001b[0m | \u001b[0m 0.04601 \u001b[0m | \u001b[0m 0.1241  \u001b[0m | \u001b[0m 0.5373  \u001b[0m | \u001b[0m 0.2035  \u001b[0m | \u001b[0m 0.9029  \u001b[0m | \u001b[0m 0.8565  \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m 0.9106  \u001b[0m | \u001b[0m 0.316   \u001b[0m | \u001b[0m 0.9705  \u001b[0m | \u001b[0m 0.3545  \u001b[0m | \u001b[0m 0.343   \u001b[0m | \u001b[0m 0.7028  \u001b[0m | \u001b[0m 0.3518  \u001b[0m |\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m 0.9145  \u001b[0m | \u001b[0m 0.4374  \u001b[0m | \u001b[0m 0.8323  \u001b[0m | \u001b[0m 0.05771 \u001b[0m | \u001b[0m 0.6559  \u001b[0m | \u001b[0m 0.1392  \u001b[0m | \u001b[0m 0.3003  \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m 0.9071  \u001b[0m | \u001b[0m 0.8657  \u001b[0m | \u001b[0m 0.5289  \u001b[0m | \u001b[0m 0.6096  \u001b[0m | \u001b[0m 0.08255 \u001b[0m | \u001b[0m 0.984   \u001b[0m | \u001b[0m 0.2535  \u001b[0m |\n",
      "| \u001b[95m 30      \u001b[0m | \u001b[95m 0.9155  \u001b[0m | \u001b[95m 0.3308  \u001b[0m | \u001b[95m 0.3715  \u001b[0m | \u001b[95m 0.1659  \u001b[0m | \u001b[95m 0.594   \u001b[0m | \u001b[95m 0.01897 \u001b[0m | \u001b[95m 0.3224  \u001b[0m |\n",
      "| \u001b[0m 31      \u001b[0m | \u001b[0m 0.9132  \u001b[0m | \u001b[0m 0.08686 \u001b[0m | \u001b[0m 0.1616  \u001b[0m | \u001b[0m 0.5338  \u001b[0m | \u001b[0m 0.4484  \u001b[0m | \u001b[0m 0.1553  \u001b[0m | \u001b[0m 0.1164  \u001b[0m |\n",
      "| \u001b[0m 32      \u001b[0m | \u001b[0m 0.9063  \u001b[0m | \u001b[0m 0.2245  \u001b[0m | \u001b[0m 0.5341  \u001b[0m | \u001b[0m 0.8502  \u001b[0m | \u001b[0m 0.01321 \u001b[0m | \u001b[0m 0.5556  \u001b[0m | \u001b[0m 0.2472  \u001b[0m |\n",
      "| \u001b[0m 33      \u001b[0m | \u001b[0m 0.9109  \u001b[0m | \u001b[0m 0.5902  \u001b[0m | \u001b[0m 0.8943  \u001b[0m | \u001b[0m 0.7477  \u001b[0m | \u001b[0m 0.3465  \u001b[0m | \u001b[0m 0.6296  \u001b[0m | \u001b[0m 0.4998  \u001b[0m |\n",
      "| \u001b[0m 34      \u001b[0m | \u001b[0m 0.9127  \u001b[0m | \u001b[0m 0.8473  \u001b[0m | \u001b[0m 0.6972  \u001b[0m | \u001b[0m 0.3802  \u001b[0m | \u001b[0m 0.423   \u001b[0m | \u001b[0m 0.08915 \u001b[0m | \u001b[0m 0.3033  \u001b[0m |\n",
      "| \u001b[0m 35      \u001b[0m | \u001b[0m 0.9074  \u001b[0m | \u001b[0m 0.8029  \u001b[0m | \u001b[0m 0.1863  \u001b[0m | \u001b[0m 0.7412  \u001b[0m | \u001b[0m 0.1075  \u001b[0m | \u001b[0m 0.6458  \u001b[0m | \u001b[0m 0.5046  \u001b[0m |\n",
      "| \u001b[0m 36      \u001b[0m | \u001b[0m 0.9079  \u001b[0m | \u001b[0m 0.965   \u001b[0m | \u001b[0m 0.9442  \u001b[0m | \u001b[0m 0.4207  \u001b[0m | \u001b[0m 0.00818 \u001b[0m | \u001b[0m 0.253   \u001b[0m | \u001b[0m 0.4405  \u001b[0m |\n",
      "| \u001b[0m 37      \u001b[0m | \u001b[0m 0.9103  \u001b[0m | \u001b[0m 0.4914  \u001b[0m | \u001b[0m 0.8675  \u001b[0m | \u001b[0m 0.2863  \u001b[0m | \u001b[0m 0.286   \u001b[0m | \u001b[0m 0.7538  \u001b[0m | \u001b[0m 0.1173  \u001b[0m |\n",
      "| \u001b[0m 38      \u001b[0m | \u001b[0m 0.9136  \u001b[0m | \u001b[0m 0.8908  \u001b[0m | \u001b[0m 0.7134  \u001b[0m | \u001b[0m 0.442   \u001b[0m | \u001b[0m 0.9889  \u001b[0m | \u001b[0m 0.7685  \u001b[0m | \u001b[0m 0.8214  \u001b[0m |\n",
      "| \u001b[0m 39      \u001b[0m | \u001b[0m 0.913   \u001b[0m | \u001b[0m 0.7904  \u001b[0m | \u001b[0m 0.8877  \u001b[0m | \u001b[0m 0.9771  \u001b[0m | \u001b[0m 0.7754  \u001b[0m | \u001b[0m 0.4279  \u001b[0m | \u001b[0m 0.7928  \u001b[0m |\n",
      "| \u001b[0m 40      \u001b[0m | \u001b[0m 0.9095  \u001b[0m | \u001b[0m 0.8807  \u001b[0m | \u001b[0m 0.9612  \u001b[0m | \u001b[0m 0.2086  \u001b[0m | \u001b[0m 0.1299  \u001b[0m | \u001b[0m 0.2041  \u001b[0m | \u001b[0m 0.7591  \u001b[0m |\n",
      "| \u001b[0m 41      \u001b[0m | \u001b[0m 0.9114  \u001b[0m | \u001b[0m 0.897   \u001b[0m | \u001b[0m 0.7243  \u001b[0m | \u001b[0m 0.3264  \u001b[0m | \u001b[0m 0.4322  \u001b[0m | \u001b[0m 0.6321  \u001b[0m | \u001b[0m 0.7504  \u001b[0m |\n",
      "| \u001b[0m 42      \u001b[0m | \u001b[0m 0.9119  \u001b[0m | \u001b[0m 0.4419  \u001b[0m | \u001b[0m 0.07668 \u001b[0m | \u001b[0m 0.5691  \u001b[0m | \u001b[0m 0.4288  \u001b[0m | \u001b[0m 0.08166 \u001b[0m | \u001b[0m 0.9602  \u001b[0m |\n",
      "| \u001b[0m 43      \u001b[0m | \u001b[0m 0.9068  \u001b[0m | \u001b[0m 0.04868 \u001b[0m | \u001b[0m 0.4564  \u001b[0m | \u001b[0m 0.1984  \u001b[0m | \u001b[0m 0.2133  \u001b[0m | \u001b[0m 0.7203  \u001b[0m | \u001b[0m 0.7968  \u001b[0m |\n",
      "| \u001b[0m 44      \u001b[0m | \u001b[0m 0.9097  \u001b[0m | \u001b[0m 0.8847  \u001b[0m | \u001b[0m 0.7949  \u001b[0m | \u001b[0m 0.7228  \u001b[0m | \u001b[0m 0.1351  \u001b[0m | \u001b[0m 0.1499  \u001b[0m | \u001b[0m 0.1431  \u001b[0m |\n",
      "| \u001b[0m 45      \u001b[0m | \u001b[0m 0.9121  \u001b[0m | \u001b[0m 0.797   \u001b[0m | \u001b[0m 0.7623  \u001b[0m | \u001b[0m 0.309   \u001b[0m | \u001b[0m 0.5469  \u001b[0m | \u001b[0m 0.7829  \u001b[0m | \u001b[0m 0.1401  \u001b[0m |\n",
      "| \u001b[0m 46      \u001b[0m | \u001b[0m 0.914   \u001b[0m | \u001b[0m 0.9724  \u001b[0m | \u001b[0m 0.6882  \u001b[0m | \u001b[0m 0.6238  \u001b[0m | \u001b[0m 0.8095  \u001b[0m | \u001b[0m 0.1015  \u001b[0m | \u001b[0m 0.09596 \u001b[0m |\n",
      "| \u001b[0m 47      \u001b[0m | \u001b[0m 0.9102  \u001b[0m | \u001b[0m 0.4142  \u001b[0m | \u001b[0m 0.5221  \u001b[0m | \u001b[0m 0.5057  \u001b[0m | \u001b[0m 0.1189  \u001b[0m | \u001b[0m 0.1046  \u001b[0m | \u001b[0m 0.475   \u001b[0m |\n",
      "| \u001b[0m 48      \u001b[0m | \u001b[0m 0.9134  \u001b[0m | \u001b[0m 0.5886  \u001b[0m | \u001b[0m 0.9158  \u001b[0m | \u001b[0m 0.2624  \u001b[0m | \u001b[0m 0.9556  \u001b[0m | \u001b[0m 0.9093  \u001b[0m | \u001b[0m 0.6617  \u001b[0m |\n",
      "| \u001b[0m 49      \u001b[0m | \u001b[0m 0.9123  \u001b[0m | \u001b[0m 0.2013  \u001b[0m | \u001b[0m 0.6768  \u001b[0m | \u001b[0m 0.01746 \u001b[0m | \u001b[0m 0.3081  \u001b[0m | \u001b[0m 0.2926  \u001b[0m | \u001b[0m 0.2537  \u001b[0m |\n",
      "| \u001b[0m 50      \u001b[0m | \u001b[0m 0.9102  \u001b[0m | \u001b[0m 0.85    \u001b[0m | \u001b[0m 0.7214  \u001b[0m | \u001b[0m 0.3346  \u001b[0m | \u001b[0m 0.3165  \u001b[0m | \u001b[0m 0.8097  \u001b[0m | \u001b[0m 0.03328 \u001b[0m |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 51      \u001b[0m | \u001b[0m 0.9125  \u001b[0m | \u001b[0m 0.02108 \u001b[0m | \u001b[0m 0.894   \u001b[0m | \u001b[0m 0.08819 \u001b[0m | \u001b[0m 0.9951  \u001b[0m | \u001b[0m 0.08939 \u001b[0m | \u001b[0m 0.07273 \u001b[0m |\n",
      "| \u001b[0m 52      \u001b[0m | \u001b[0m 0.9147  \u001b[0m | \u001b[0m 0.9245  \u001b[0m | \u001b[0m 0.6684  \u001b[0m | \u001b[0m 0.004499\u001b[0m | \u001b[0m 0.9998  \u001b[0m | \u001b[0m 0.2316  \u001b[0m | \u001b[0m 0.07711 \u001b[0m |\n",
      "| \u001b[0m 53      \u001b[0m | \u001b[0m 0.915   \u001b[0m | \u001b[0m 0.8854  \u001b[0m | \u001b[0m 0.2104  \u001b[0m | \u001b[0m 0.01595 \u001b[0m | \u001b[0m 0.9962  \u001b[0m | \u001b[0m 0.03171 \u001b[0m | \u001b[0m 0.6208  \u001b[0m |\n",
      "| \u001b[0m 54      \u001b[0m | \u001b[0m 0.9145  \u001b[0m | \u001b[0m 0.2511  \u001b[0m | \u001b[0m 0.053   \u001b[0m | \u001b[0m 0.9619  \u001b[0m | \u001b[0m 0.9284  \u001b[0m | \u001b[0m 0.06444 \u001b[0m | \u001b[0m 0.2905  \u001b[0m |\n",
      "| \u001b[0m 55      \u001b[0m | \u001b[0m 0.9149  \u001b[0m | \u001b[0m 0.9802  \u001b[0m | \u001b[0m 0.4167  \u001b[0m | \u001b[0m 0.07708 \u001b[0m | \u001b[0m 0.9833  \u001b[0m | \u001b[0m 0.09387 \u001b[0m | \u001b[0m 0.1461  \u001b[0m |\n",
      "| \u001b[0m 56      \u001b[0m | \u001b[0m 0.9155  \u001b[0m | \u001b[0m 0.2385  \u001b[0m | \u001b[0m 0.04785 \u001b[0m | \u001b[0m 0.3362  \u001b[0m | \u001b[0m 0.9848  \u001b[0m | \u001b[0m 0.06586 \u001b[0m | \u001b[0m 0.009867\u001b[0m |\n",
      "| \u001b[0m 57      \u001b[0m | \u001b[0m 0.9143  \u001b[0m | \u001b[0m 0.8071  \u001b[0m | \u001b[0m 0.7237  \u001b[0m | \u001b[0m 0.8904  \u001b[0m | \u001b[0m 0.983   \u001b[0m | \u001b[0m 0.04411 \u001b[0m | \u001b[0m 0.01496 \u001b[0m |\n",
      "| \u001b[0m 58      \u001b[0m | \u001b[0m 0.915   \u001b[0m | \u001b[0m 0.3395  \u001b[0m | \u001b[0m 0.7845  \u001b[0m | \u001b[0m 0.1558  \u001b[0m | \u001b[0m 0.9649  \u001b[0m | \u001b[0m 0.001855\u001b[0m | \u001b[0m 0.04567 \u001b[0m |\n",
      "| \u001b[0m 59      \u001b[0m | \u001b[0m 0.9147  \u001b[0m | \u001b[0m 0.1662  \u001b[0m | \u001b[0m 0.125   \u001b[0m | \u001b[0m 0.04722 \u001b[0m | \u001b[0m 0.9923  \u001b[0m | \u001b[0m 0.1455  \u001b[0m | \u001b[0m 0.9338  \u001b[0m |\n",
      "| \u001b[0m 60      \u001b[0m | \u001b[0m 0.9141  \u001b[0m | \u001b[0m 0.9857  \u001b[0m | \u001b[0m 0.05115 \u001b[0m | \u001b[0m 0.07362 \u001b[0m | \u001b[0m 0.9782  \u001b[0m | \u001b[0m 0.1099  \u001b[0m | \u001b[0m 0.01828 \u001b[0m |\n",
      "| \u001b[0m 61      \u001b[0m | \u001b[0m 0.9151  \u001b[0m | \u001b[0m 0.4134  \u001b[0m | \u001b[0m 0.3238  \u001b[0m | \u001b[0m 0.6366  \u001b[0m | \u001b[0m 0.9908  \u001b[0m | \u001b[0m 0.000830\u001b[0m | \u001b[0m 0.7609  \u001b[0m |\n",
      "| \u001b[0m 62      \u001b[0m | \u001b[0m 0.9132  \u001b[0m | \u001b[0m 0.08215 \u001b[0m | \u001b[0m 0.9763  \u001b[0m | \u001b[0m 0.9594  \u001b[0m | \u001b[0m 0.9809  \u001b[0m | \u001b[0m 0.009327\u001b[0m | \u001b[0m 0.5677  \u001b[0m |\n",
      "| \u001b[0m 63      \u001b[0m | \u001b[0m 0.914   \u001b[0m | \u001b[0m 0.03714 \u001b[0m | \u001b[0m 0.1153  \u001b[0m | \u001b[0m 0.2058  \u001b[0m | \u001b[0m 0.9618  \u001b[0m | \u001b[0m 0.02096 \u001b[0m | \u001b[0m 0.03688 \u001b[0m |\n",
      "| \u001b[0m 64      \u001b[0m | \u001b[0m 0.9146  \u001b[0m | \u001b[0m 0.8127  \u001b[0m | \u001b[0m 0.1194  \u001b[0m | \u001b[0m 0.08581 \u001b[0m | \u001b[0m 0.9921  \u001b[0m | \u001b[0m 0.0351  \u001b[0m | \u001b[0m 0.965   \u001b[0m |\n",
      "| \u001b[0m 65      \u001b[0m | \u001b[0m 0.9146  \u001b[0m | \u001b[0m 0.6976  \u001b[0m | \u001b[0m 0.6494  \u001b[0m | \u001b[0m 0.7019  \u001b[0m | \u001b[0m 0.9966  \u001b[0m | \u001b[0m 0.003778\u001b[0m | \u001b[0m 0.9294  \u001b[0m |\n",
      "| \u001b[0m 66      \u001b[0m | \u001b[0m 0.914   \u001b[0m | \u001b[0m 0.7381  \u001b[0m | \u001b[0m 0.02914 \u001b[0m | \u001b[0m 0.9116  \u001b[0m | \u001b[0m 0.9903  \u001b[0m | \u001b[0m 0.2859  \u001b[0m | \u001b[0m 0.003234\u001b[0m |\n",
      "| \u001b[95m 67      \u001b[0m | \u001b[95m 0.9159  \u001b[0m | \u001b[95m 0.2919  \u001b[0m | \u001b[95m 0.0212  \u001b[0m | \u001b[95m 0.0209  \u001b[0m | \u001b[95m 0.993   \u001b[0m | \u001b[95m 0.03749 \u001b[0m | \u001b[95m 0.6001  \u001b[0m |\n",
      "| \u001b[0m 68      \u001b[0m | \u001b[0m 0.9148  \u001b[0m | \u001b[0m 0.6319  \u001b[0m | \u001b[0m 0.813   \u001b[0m | \u001b[0m 0.007836\u001b[0m | \u001b[0m 0.9405  \u001b[0m | \u001b[0m 0.006215\u001b[0m | \u001b[0m 0.9911  \u001b[0m |\n",
      "| \u001b[0m 69      \u001b[0m | \u001b[0m 0.9146  \u001b[0m | \u001b[0m 0.02168 \u001b[0m | \u001b[0m 0.05858 \u001b[0m | \u001b[0m 0.005655\u001b[0m | \u001b[0m 0.9941  \u001b[0m | \u001b[0m 0.07428 \u001b[0m | \u001b[0m 0.4189  \u001b[0m |\n",
      "| \u001b[95m 70      \u001b[0m | \u001b[95m 0.9159  \u001b[0m | \u001b[95m 0.4315  \u001b[0m | \u001b[95m 0.05224 \u001b[0m | \u001b[95m 0.005699\u001b[0m | \u001b[95m 0.9641  \u001b[0m | \u001b[95m 0.0222  \u001b[0m | \u001b[95m 0.5128  \u001b[0m |\n",
      "| \u001b[0m 71      \u001b[0m | \u001b[0m 0.9154  \u001b[0m | \u001b[0m 0.5705  \u001b[0m | \u001b[0m 0.3647  \u001b[0m | \u001b[0m 0.5587  \u001b[0m | \u001b[0m 0.9971  \u001b[0m | \u001b[0m 0.01182 \u001b[0m | \u001b[0m 0.1579  \u001b[0m |\n",
      "| \u001b[0m 72      \u001b[0m | \u001b[0m 0.9156  \u001b[0m | \u001b[0m 0.4084  \u001b[0m | \u001b[0m 0.3886  \u001b[0m | \u001b[0m 0.3624  \u001b[0m | \u001b[0m 0.9812  \u001b[0m | \u001b[0m 0.0929  \u001b[0m | \u001b[0m 0.5029  \u001b[0m |\n",
      "| \u001b[0m 73      \u001b[0m | \u001b[0m 0.9158  \u001b[0m | \u001b[0m 0.4351  \u001b[0m | \u001b[0m 0.4358  \u001b[0m | \u001b[0m 0.2751  \u001b[0m | \u001b[0m 0.9926  \u001b[0m | \u001b[0m 0.09928 \u001b[0m | \u001b[0m 0.3654  \u001b[0m |\n",
      "| \u001b[0m 74      \u001b[0m | \u001b[0m 0.9154  \u001b[0m | \u001b[0m 0.5481  \u001b[0m | \u001b[0m 0.08065 \u001b[0m | \u001b[0m 0.3384  \u001b[0m | \u001b[0m 0.9951  \u001b[0m | \u001b[0m 0.01983 \u001b[0m | \u001b[0m 0.7552  \u001b[0m |\n",
      "| \u001b[0m 75      \u001b[0m | \u001b[0m 0.9157  \u001b[0m | \u001b[0m 0.5518  \u001b[0m | \u001b[0m 0.017   \u001b[0m | \u001b[0m 0.0198  \u001b[0m | \u001b[0m 0.9798  \u001b[0m | \u001b[0m 0.05848 \u001b[0m | \u001b[0m 0.4968  \u001b[0m |\n",
      "| \u001b[0m 76      \u001b[0m | \u001b[0m 0.9154  \u001b[0m | \u001b[0m 0.5793  \u001b[0m | \u001b[0m 0.1017  \u001b[0m | \u001b[0m 0.004479\u001b[0m | \u001b[0m 0.9919  \u001b[0m | \u001b[0m 0.06795 \u001b[0m | \u001b[0m 0.7318  \u001b[0m |\n",
      "| \u001b[0m 77      \u001b[0m | \u001b[0m 0.9148  \u001b[0m | \u001b[0m 0.8142  \u001b[0m | \u001b[0m 0.01877 \u001b[0m | \u001b[0m 0.05226 \u001b[0m | \u001b[0m 0.9915  \u001b[0m | \u001b[0m 0.04867 \u001b[0m | \u001b[0m 0.1668  \u001b[0m |\n",
      "| \u001b[0m 78      \u001b[0m | \u001b[0m 0.9154  \u001b[0m | \u001b[0m 0.5661  \u001b[0m | \u001b[0m 0.727   \u001b[0m | \u001b[0m 0.1297  \u001b[0m | \u001b[0m 0.9895  \u001b[0m | \u001b[0m 0.03872 \u001b[0m | \u001b[0m 0.5925  \u001b[0m |\n",
      "| \u001b[0m 79      \u001b[0m | \u001b[0m 0.9154  \u001b[0m | \u001b[0m 0.4962  \u001b[0m | \u001b[0m 0.3632  \u001b[0m | \u001b[0m 0.4932  \u001b[0m | \u001b[0m 0.9866  \u001b[0m | \u001b[0m 0.05272 \u001b[0m | \u001b[0m 0.524   \u001b[0m |\n",
      "| \u001b[0m 80      \u001b[0m | \u001b[0m 0.9156  \u001b[0m | \u001b[0m 0.3179  \u001b[0m | \u001b[0m 0.2215  \u001b[0m | \u001b[0m 0.4706  \u001b[0m | \u001b[0m 0.9991  \u001b[0m | \u001b[0m 0.02247 \u001b[0m | \u001b[0m 0.5672  \u001b[0m |\n",
      "| \u001b[0m 81      \u001b[0m | \u001b[0m 0.9157  \u001b[0m | \u001b[0m 0.4514  \u001b[0m | \u001b[0m 0.01083 \u001b[0m | \u001b[0m 0.1661  \u001b[0m | \u001b[0m 0.9823  \u001b[0m | \u001b[0m 0.001995\u001b[0m | \u001b[0m 0.03277 \u001b[0m |\n",
      "| \u001b[95m 82      \u001b[0m | \u001b[95m 0.9159  \u001b[0m | \u001b[95m 0.5576  \u001b[0m | \u001b[95m 0.3807  \u001b[0m | \u001b[95m 0.03627 \u001b[0m | \u001b[95m 0.9987  \u001b[0m | \u001b[95m 0.00686 \u001b[0m | \u001b[95m 0.463   \u001b[0m |\n",
      "| \u001b[0m 83      \u001b[0m | \u001b[0m 0.9148  \u001b[0m | \u001b[0m 0.8549  \u001b[0m | \u001b[0m 0.8278  \u001b[0m | \u001b[0m 0.1294  \u001b[0m | \u001b[0m 0.9782  \u001b[0m | \u001b[0m 0.07964 \u001b[0m | \u001b[0m 0.06488 \u001b[0m |\n",
      "| \u001b[0m 84      \u001b[0m | \u001b[0m 0.9158  \u001b[0m | \u001b[0m 0.5723  \u001b[0m | \u001b[0m 0.3704  \u001b[0m | \u001b[0m 0.2924  \u001b[0m | \u001b[0m 0.9939  \u001b[0m | \u001b[0m 0.01516 \u001b[0m | \u001b[0m 0.3359  \u001b[0m |\n",
      "| \u001b[0m 85      \u001b[0m | \u001b[0m 0.9158  \u001b[0m | \u001b[0m 0.2941  \u001b[0m | \u001b[0m 0.1893  \u001b[0m | \u001b[0m 0.1345  \u001b[0m | \u001b[0m 0.9941  \u001b[0m | \u001b[0m 0.05313 \u001b[0m | \u001b[0m 0.03226 \u001b[0m |\n",
      "| \u001b[0m 86      \u001b[0m | \u001b[0m 0.9151  \u001b[0m | \u001b[0m 0.5566  \u001b[0m | \u001b[0m 0.06112 \u001b[0m | \u001b[0m 0.8594  \u001b[0m | \u001b[0m 0.9962  \u001b[0m | \u001b[0m 0.03397 \u001b[0m | \u001b[0m 0.3825  \u001b[0m |\n",
      "| \u001b[0m 87      \u001b[0m | \u001b[0m 0.9156  \u001b[0m | \u001b[0m 0.5661  \u001b[0m | \u001b[0m 0.5114  \u001b[0m | \u001b[0m 0.135   \u001b[0m | \u001b[0m 0.9977  \u001b[0m | \u001b[0m 0.01338 \u001b[0m | \u001b[0m 0.6613  \u001b[0m |\n",
      "| \u001b[0m 88      \u001b[0m | \u001b[0m 0.9158  \u001b[0m | \u001b[0m 0.4175  \u001b[0m | \u001b[0m 0.3742  \u001b[0m | \u001b[0m 0.2289  \u001b[0m | \u001b[0m 0.998   \u001b[0m | \u001b[0m 0.06198 \u001b[0m | \u001b[0m 0.5183  \u001b[0m |\n",
      "| \u001b[0m 89      \u001b[0m | \u001b[0m 0.9158  \u001b[0m | \u001b[0m 0.6251  \u001b[0m | \u001b[0m 0.2674  \u001b[0m | \u001b[0m 0.3224  \u001b[0m | \u001b[0m 0.9954  \u001b[0m | \u001b[0m 0.003575\u001b[0m | \u001b[0m 0.3264  \u001b[0m |\n",
      "| \u001b[0m 90      \u001b[0m | \u001b[0m 0.9159  \u001b[0m | \u001b[0m 0.4748  \u001b[0m | \u001b[0m 0.04951 \u001b[0m | \u001b[0m 0.3033  \u001b[0m | \u001b[0m 0.996   \u001b[0m | \u001b[0m 0.006284\u001b[0m | \u001b[0m 0.4876  \u001b[0m |\n",
      "=================================================================================================\n",
      "init_points = 50, n_iter = 40, CV:0.9159\n"
     ]
    }
   ],
   "source": [
    "prediction_test = fitBayesian(MVTable, TestTable, init_points=50, n_iter=40)\n",
    "\n",
    "csv_file_path = Output + 'submit/sub_stacking.csv'\n",
    "sub = pd.read_csv(SIIM_ISIC_Melanoma_Classification + 'sample_submission.csv')\n",
    "\n",
    "sub.target = prediction_test\n",
    "sub.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 374k/374k [00:03<00:00, 108kB/s]  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Successfully submitted to SIIM-ISIC Melanoma Classification"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#submit\n",
    "api = KaggleApi()\n",
    "api.authenticate()  # 認証を通す\n",
    "message = \"blending BayesianOptimization \"\n",
    "competition_id = 'siim-isic-melanoma-classification'\n",
    "api.competition_submit(csv_file_path, message, competition_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bayes test end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "import time\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_classification(X, X_test, y, params, folds, model_type='lgb', eval_metric='auc', columns=None, \n",
    "                               plot_feature_importance=False, model=None, verbose=10000, early_stopping_rounds=200, n_estimators=50000):\n",
    "    \"\"\"\n",
    "    A function to train a variety of classification models.\n",
    "    Returns dictionary with oof predictions, test predictions, scores and, if necessary, feature importances.\n",
    "    \n",
    "    :params: X - training data, can be pd.DataFrame or np.ndarray (after normalizing)\n",
    "    :params: X_test - test data, can be pd.DataFrame or np.ndarray (after normalizing)\n",
    "    :params: y - target\n",
    "    :params: folds - folds to split data\n",
    "    :params: model_type - type of model to use\n",
    "    :params: eval_metric - metric to use\n",
    "    :params: columns - columns to use. If None - use all columns\n",
    "    :params: plot_feature_importance - whether to plot feature importance of LGB\n",
    "    :params: model - sklearn model, works only for \"sklearn\" model type\n",
    "    :params: verbose - parameters for gradient boosting models\n",
    "    :params: early_stopping_rounds - parameters for gradient boosting models\n",
    "    :params: n_estimators - parameters for gradient boosting models\n",
    "    \n",
    "    \"\"\"\n",
    "    columns = X.columns if columns == None else columns\n",
    "    X_test = X_test[columns]\n",
    "    #X_metaval = X_metaval[columns]\n",
    "    \n",
    "    # to set up scoring parameters\n",
    "    metrics_dict = {'auc': {'lgb_metric_name': 'auc',\n",
    "                        'catboost_metric_name': 'AUC',\n",
    "                        'sklearn_scoring_function': metrics.roc_auc_score},\n",
    "                    }\n",
    "    \n",
    "    result_dict = {}\n",
    "    \n",
    "    # out-of-fold predictions on train data\n",
    "    oof = np.zeros((len(X), len(set(y.values))))\n",
    "    \n",
    "    # averaged predictions on train data\n",
    "    prediction = np.zeros((len(X_test), oof.shape[1]))\n",
    "    #prediction_MV = np.zeros((len(X_metaval), oof.shape[1]))\n",
    "    \n",
    "    # list of scores on folds\n",
    "    scores = []\n",
    "    feature_importance = pd.DataFrame()\n",
    "    \n",
    "    # split and train on folds\n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n",
    "        print(f'Fold {fold_n + 1} started at {time.ctime()}')\n",
    "        if type(X) == np.ndarray:\n",
    "            X_train, X_valid = X[columns][train_index], X[columns][valid_index]\n",
    "            y_train, y_valid = y[train_index], y[valid_index]\n",
    "        else:\n",
    "            X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n",
    "            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "            \n",
    "        if model_type == 'lgb':\n",
    "            model = lgb.LGBMClassifier(**params, n_estimators=n_estimators, n_jobs = -1)\n",
    "            model.fit(X_train, y_train, \n",
    "                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric=metrics_dict[eval_metric]['lgb_metric_name'],\n",
    "                    verbose=verbose, early_stopping_rounds=early_stopping_rounds)\n",
    "            \n",
    "            y_pred_valid = model.predict_proba(X_valid, num_iteration=model.best_iteration_)\n",
    "            y_pred = model.predict_proba(X_test, num_iteration=model.best_iteration_)\n",
    "            #y_pred_MV = model.predict_proba(X_metaval, num_iteration=model.best_iteration_)\n",
    "            \n",
    "        if model_type == 'xgb':\n",
    "            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n",
    "            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n",
    "\n",
    "            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n",
    "            model = xgb.train(dtrain=train_data, num_boost_round=n_estimators, evals=watchlist, \n",
    "                              early_stopping_rounds=early_stopping_rounds, verbose_eval=verbose, params=params)\n",
    "            y_pred_valid = model.predict_proba(xgb.DMatrix(X_valid, feature_names=X.columns), \n",
    "                                         ntree_limit=model.best_ntree_limit)\n",
    "            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
    "            #y_pred_MV = model.predict(xgb.DMatrix(X_metaval, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
    "        \n",
    "        if model_type == 'sklearn':\n",
    "            model = model\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            y_pred_valid = model.predict_proba(X_valid)#.reshape(-1,)\n",
    "            #print(len(y_pred_valid))\n",
    "            #print(len(y_valid))\n",
    "            #print(y_pred_valid)\n",
    "            #print(y_valid)\n",
    "            score = metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid[:, 1])\n",
    "            print(f'Fold {fold_n + 1}. {eval_metric}: {score:.4f}.')\n",
    "            print('')\n",
    "            \n",
    "            y_pred = model.predict_proba(X_test)\n",
    "            #y_pred_MV = model.predict_proba(X_metaval)\n",
    "        \n",
    "        if model_type == 'cat':\n",
    "            model = CatBoostClassifier(iterations=n_estimators, eval_metric=metrics_dict[eval_metric]['catboost_metric_name'], **params,\n",
    "                                      loss_function=metrics_dict[eval_metric]['catboost_metric_name'])\n",
    "            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n",
    "\n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test)\n",
    "            #y_pred_MV = model.predict(X_metaval)\n",
    "        \n",
    "        #print(oof.shape)\n",
    "        #print(oof[valid_index])\n",
    "        oof[valid_index] = y_pred_valid\n",
    "        scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid[:, 1]))#0 or 1の問題にしか対応してない\n",
    "\n",
    "        prediction += y_pred    \n",
    "        #prediction_MV += y_pred_MV    \n",
    "        \n",
    "        if model_type == 'lgb' and plot_feature_importance:\n",
    "            # feature importance\n",
    "            fold_importance = pd.DataFrame()\n",
    "            fold_importance[\"feature\"] = columns\n",
    "            fold_importance[\"importance\"] = model.feature_importances_\n",
    "            fold_importance[\"fold\"] = fold_n + 1\n",
    "            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n",
    "\n",
    "    prediction /= folds.n_splits\n",
    "    #prediction_MV /= folds.n_splits\n",
    "    \n",
    "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "    \n",
    "    result_dict['oof'] = oof\n",
    "    result_dict['prediction'] = prediction\n",
    "    #result_dict['prediction_MV'] = prediction_MV\n",
    "    result_dict['scores'] = scores\n",
    "    \n",
    "    if model_type == 'lgb':\n",
    "        if plot_feature_importance:\n",
    "            feature_importance[\"importance\"] /= folds.n_splits\n",
    "            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
    "                by=\"importance\", ascending=False)[:50].index\n",
    "\n",
    "            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n",
    "\n",
    "            plt.figure(figsize=(16, 12));\n",
    "            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n",
    "            plt.title('LGB Features (avg over folds)');\n",
    "            \n",
    "            result_dict['feature_importance'] = feature_importance\n",
    "        \n",
    "    return result_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__run__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 started at Sun Jul 19 16:44:41 2020\n",
      "Fold 1. auc: 0.8720.\n",
      "\n",
      "Fold 2 started at Sun Jul 19 16:44:41 2020\n",
      "Fold 2. auc: 0.7798.\n",
      "\n",
      "Fold 3 started at Sun Jul 19 16:44:41 2020\n",
      "Fold 3. auc: 0.8699.\n",
      "\n",
      "Fold 4 started at Sun Jul 19 16:44:42 2020\n",
      "Fold 4. auc: 0.7029.\n",
      "\n",
      "Fold 5 started at Sun Jul 19 16:44:42 2020\n",
      "Fold 5. auc: 0.8268.\n",
      "\n",
      "CV mean score: 0.8103, std: 0.0634.\n",
      "CPU times: user 1.3 s, sys: 5.76 ms, total: 1.31 s\n",
      "Wall time: 1.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#SVM\n",
    "#交差検証\n",
    "n_fold = 5\n",
    "folds = KFold(n_splits=n_fold, shuffle=True, random_state=11)\n",
    "m_type = \"sklearn\"\n",
    "\n",
    "params = None\n",
    "C = 0.09\n",
    "gamma = 'auto'\n",
    "\n",
    "#data\n",
    "y = MVTable[\"target\"]\n",
    "X = MVTable.drop(['target','image_name'], axis=1)\n",
    "X_test = TestTable.drop(['image_name'], axis=1)\n",
    "\n",
    "#support vector machine\n",
    "model = SVC(C=C, gamma=gamma, probability=True)\n",
    "result_dict = train_model_classification(X=X, X_test=X_test, y=y, params=params, folds=folds, model_type=m_type, eval_metric='auc',\n",
    "                                         plot_feature_importance=True, verbose=1000, early_stopping_rounds=200, n_estimators=10000, model = model)\n",
    "\n",
    "csv_file_path = Output + 'submit/sub_blending.csv'\n",
    "sub = pd.read_csv(SIIM_ISIC_Melanoma_Classification + 'sample_submission.csv')\n",
    "\n",
    "sub.target = result_dict[\"prediction\"][:,1]\n",
    "sub.to_csv(csv_file_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 started at Sun Jul 19 16:00:36 2020\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LinearRegression' object has no attribute 'predict_proba'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-c4976f46786d>\u001b[0m in \u001b[0;36mtrain_model_classification\u001b[0;34m(X, X_test, y, params, folds, model_type, eval_metric, columns, plot_feature_importance, model, verbose, early_stopping_rounds, n_estimators)\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0my_pred_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.reshape(-1,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0;31m#print(len(y_pred_valid))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;31m#print(len(y_valid))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LinearRegression' object has no attribute 'predict_proba'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Linear\n",
    "#交差検証\n",
    "n_fold = 5\n",
    "folds = KFold(n_splits=n_fold, shuffle=True, random_state=11)\n",
    "m_type = \"sklearn\"\n",
    "\n",
    "params = None\n",
    "\n",
    "#data\n",
    "y = MVTable[\"target\"]\n",
    "X = MVTable.drop(['target','image_name'], axis=1)\n",
    "X_test = TestTable.drop(['image_name'], axis=1)\n",
    "\n",
    "#support vector machine\n",
    "model = LinearRegression()\n",
    "result_dict = train_model_classification(X=X, X_test=X_test, y=y, params=params, folds=folds, model_type=m_type, eval_metric='auc',\n",
    "                                         plot_feature_importance=True, verbose=1000, early_stopping_rounds=200, n_estimators=10000, model = model)\n",
    "\n",
    "csv_file_path = Output + 'submit/sub_blending.csv'\n",
    "sub = pd.read_csv(SIIM_ISIC_Melanoma_Classification + 'sample_submission.csv')\n",
    "\n",
    "sub.target = result_dict[\"prediction\"][:,1]\n",
    "sub.to_csv(csv_file_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 started at Sun Jul 19 15:42:02 2020\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[6]\ttraining's binary_logloss: 0.0795649\ttraining's auc: 0.964909\tvalid_1's binary_logloss: 0.0800094\tvalid_1's auc: 0.885671\n",
      "Fold 2 started at Sun Jul 19 15:42:02 2020\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[29]\ttraining's binary_logloss: 0.0629722\ttraining's auc: 0.978919\tvalid_1's binary_logloss: 0.0756656\tvalid_1's auc: 0.856793\n",
      "Fold 3 started at Sun Jul 19 15:42:03 2020\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[98]\ttraining's binary_logloss: 0.0393994\ttraining's auc: 0.995215\tvalid_1's binary_logloss: 0.0840326\tvalid_1's auc: 0.884971\n",
      "Fold 4 started at Sun Jul 19 15:42:03 2020\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[9]\ttraining's binary_logloss: 0.0771374\ttraining's auc: 0.962198\tvalid_1's binary_logloss: 0.0789313\tvalid_1's auc: 0.910922\n",
      "Fold 5 started at Sun Jul 19 15:42:03 2020\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[10]\ttraining's binary_logloss: 0.0765215\ttraining's auc: 0.964099\tvalid_1's binary_logloss: 0.0768518\tvalid_1's auc: 0.880558\n",
      "CV mean score: 0.8838, std: 0.0172.\n",
      "CPU times: user 20.3 s, sys: 6.48 ms, total: 20.3 s\n",
      "Wall time: 1.48 s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "#lgb\n",
    "#交差検証\n",
    "n_fold = 5\n",
    "folds = KFold(n_splits=n_fold, shuffle=True, random_state=11)\n",
    "m_type = \"lgb\"\n",
    "\n",
    "params = {\"boosting_type\": \"gbdt\",\n",
    "          \"learning_rate\": 0.01,\n",
    "          #'max_depth': 7,\n",
    "          #'num_leaves': 31,\n",
    "          \"subsample_freq\": 1,\n",
    "          \"subsample\": 0.9,\n",
    "          #'reg_alpha': 0,\n",
    "          #'reg_lambda': 0,\n",
    "         }\n",
    "\n",
    "#data\n",
    "y = MVTable[\"target\"]\n",
    "X = MVTable.drop(['target','image_name'], axis=1)\n",
    "X_test = TestTable.drop(['image_name'], axis=1)\n",
    "\n",
    "#lgb(決定木)で学習実行\n",
    "result_dict = train_model_classification(X=X, X_test=X_test, y=y, params=params, folds=folds, model_type=m_type, eval_metric='auc',\n",
    "                                         plot_feature_importance=True, verbose=1000, early_stopping_rounds=200, n_estimators=10000)\n",
    "\n",
    "\n",
    "csv_file_path = Output + 'submit/sub_blending.csv'\n",
    "sub = pd.read_csv(SIIM_ISIC_Melanoma_Classification + 'sample_submission.csv')\n",
    "\n",
    "sub.target = result_dict[\"prediction\"][:,1]\n",
    "sub.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 started at Sun Jul 19 15:54:39 2020\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'subsample_freq'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-c4976f46786d>\u001b[0m in \u001b[0;36mtrain_model_classification\u001b[0;34m(X, X_test, y, params, folds, model_type, eval_metric, columns, plot_feature_importance, model, verbose, early_stopping_rounds, n_estimators)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'cat'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             model = CatBoostClassifier(iterations=n_estimators, eval_metric=metrics_dict[eval_metric]['catboost_metric_name'], **params,\n\u001b[0;32m---> 94\u001b[0;31m                                       loss_function=metrics_dict[eval_metric]['catboost_metric_name'])\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_best_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'subsample_freq'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#xgb\n",
    "#交差検証\n",
    "n_fold = 5\n",
    "folds = KFold(n_splits=n_fold, shuffle=True, random_state=11)\n",
    "m_type = \"cat\"\n",
    "\n",
    "params = {\"boosting_type\": \"gbdt\",\n",
    "          \"learning_rate\": 0.01,\n",
    "          #'max_depth': 7,\n",
    "          #'num_leaves': 31,\n",
    "          \"subsample_freq\": 1,\n",
    "          \"subsample\": 0.9,\n",
    "          #'reg_alpha': 0,\n",
    "          #'reg_lambda': 0,\n",
    "         }\n",
    "\n",
    "#data\n",
    "y = MVTable[\"target\"]\n",
    "X = MVTable.drop(['target','image_name'], axis=1)\n",
    "X_test = TestTable.drop(['image_name'], axis=1)\n",
    "\n",
    "#lgb(決定木)で学習実行\n",
    "result_dict = train_model_classification(X=X, X_test=X_test, y=y, params=params, folds=folds, model_type=m_type, eval_metric='auc',\n",
    "                                         plot_feature_importance=True, verbose=1000, early_stopping_rounds=200, n_estimators=10000)\n",
    "\n",
    "\n",
    "csv_file_path = Output + 'submit/sub_blending.csv'\n",
    "sub = pd.read_csv(SIIM_ISIC_Melanoma_Classification + 'sample_submission.csv')\n",
    "\n",
    "sub.target = result_dict[\"prediction\"][:,1]\n",
    "sub.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 355k/355k [00:03<00:00, 91.7kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Successfully submitted to SIIM-ISIC Melanoma Classification"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#submit\n",
    "api = KaggleApi()\n",
    "api.authenticate()  # 認証を通す\n",
    "message = \"blending\"\n",
    "competition_id = 'siim-isic-melanoma-classification'\n",
    "api.competition_submit(csv_file_path, message, competition_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
