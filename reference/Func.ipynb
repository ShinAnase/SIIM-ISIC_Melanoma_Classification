{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import NuSVR, SVR\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "pd.options.display.precision = 15\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import time\n",
    "import datetime\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn import linear_model\n",
    "import gc\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from IPython.display import HTML\n",
    "import json\n",
    "import altair as alt\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "alt.renderers.enable('notebook')\n",
    "\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "import gc\n",
    "from numba import jit\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "import altair as alt\n",
    "from altair.vega import v5\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4658147 rows in train data.\n",
      "There are 2505542 rows in test data.\n",
      "There are 85003 distinct molecules in train data.\n",
      "There are 45772 distinct molecules in test data.\n",
      "There are 5 unique atoms.\n",
      "There are 8 unique types.\n"
     ]
    }
   ],
   "source": [
    "#データの読み込み、内容確認\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "sub = pd.read_csv('sample_submission.csv')\n",
    "structures = pd.read_csv('structures.csv')\n",
    "\n",
    "print(f'There are {train.shape[0]} rows in train data.')\n",
    "print(f'There are {test.shape[0]} rows in test data.')\n",
    "\n",
    "print(f\"There are {train['molecule_name'].nunique()} distinct molecules in train data.\")\n",
    "print(f\"There are {test['molecule_name'].nunique()} distinct molecules in test data.\")\n",
    "print(f\"There are {structures['atom'].nunique()} unique atoms.\")\n",
    "print(f\"There are {train['type'].nunique()} unique types.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#特徴量エンジニアリング\n",
    "\n",
    "#データとstructureを統合。\n",
    "#一つのデータにそれぞれの原子の位置をインプット。\n",
    "def map_atom_info(df, atom_idx):\n",
    "    df = pd.merge(df, structures, how = 'left',\n",
    "                  left_on  = ['molecule_name', f'atom_index_{atom_idx}'],\n",
    "                  right_on = ['molecule_name',  'atom_index'])\n",
    "    \n",
    "    df = df.drop('atom_index', axis=1)\n",
    "    df = df.rename(columns={'atom': f'atom_{atom_idx}',\n",
    "                            'x': f'x_{atom_idx}',\n",
    "                            'y': f'y_{atom_idx}',\n",
    "                            'z': f'z_{atom_idx}'})\n",
    "    return df\n",
    " \n",
    "#bruteforce\n",
    "def create_features_full(df):\n",
    "    df['molecule_couples'] = df.groupby('molecule_name')['id'].transform('count')\n",
    "    df['molecule_dist_mean'] = df.groupby('molecule_name')['dist'].transform('mean')\n",
    "    df['molecule_dist_min'] = df.groupby('molecule_name')['dist'].transform('min')\n",
    "    df['molecule_dist_max'] = df.groupby('molecule_name')['dist'].transform('max')\n",
    "    df['molecule_dist_std'] = df.groupby('molecule_name')['dist'].transform('std')\n",
    "    df['atom_0_couples_count'] = df.groupby(['molecule_name', 'atom_index_0'])['id'].transform('count')\n",
    "    df['atom_1_couples_count'] = df.groupby(['molecule_name', 'atom_index_1'])['id'].transform('count')\n",
    "\n",
    "    num_cols = ['x_1', 'y_1', 'z_1', 'dist', 'dist_x', 'dist_y', 'dist_z']\n",
    "    cat_cols = ['atom_index_0', 'atom_index_1', 'type', 'atom_1', 'type_0']\n",
    "    aggs = ['mean', 'max', 'std', 'min']\n",
    "    for col in cat_cols:\n",
    "        df[f'molecule_{col}_count'] = df.groupby('molecule_name')[col].transform('count')\n",
    "\n",
    "    for cat_col in tqdm_notebook(cat_cols):\n",
    "        for num_col in num_cols:\n",
    "            for agg in aggs:\n",
    "                df[f'molecule_{cat_col}_{num_col}_{agg}'] = df.groupby(['molecule_name', cat_col])[num_col].transform(agg)\n",
    "                df[f'molecule_{cat_col}_{num_col}_{agg}_diff'] = df[f'molecule_{cat_col}_{num_col}_{agg}'] - df[num_col]\n",
    "                df[f'molecule_{cat_col}_{num_col}_{agg}_div'] = df[f'molecule_{cat_col}_{num_col}_{agg}'] / df[num_col]\n",
    "\n",
    "    df = reduce_mem_usage(df)\n",
    "    return df\n",
    "\n",
    "#bruteforceのfeature_importanceで上位50位であった組み合わせ\n",
    "def create_features(df):\n",
    "    df['molecule_couples'] = df.groupby('molecule_name')['id'].transform('count')\n",
    "    df['molecule_dist_mean'] = df.groupby('molecule_name')['dist'].transform('mean')\n",
    "    df['molecule_dist_min'] = df.groupby('molecule_name')['dist'].transform('min')\n",
    "    df['molecule_dist_max'] = df.groupby('molecule_name')['dist'].transform('max')\n",
    "    df['atom_0_couples_count'] = df.groupby(['molecule_name', 'atom_index_0'])['id'].transform('count')\n",
    "    df['atom_1_couples_count'] = df.groupby(['molecule_name', 'atom_index_1'])['id'].transform('count')\n",
    "    \n",
    "    df[f'molecule_atom_index_0_x_1_std'] = df.groupby(['molecule_name', 'atom_index_0'])['x_1'].transform('std')\n",
    "    df[f'molecule_atom_index_0_y_1_mean'] = df.groupby(['molecule_name', 'atom_index_0'])['y_1'].transform('mean')\n",
    "    df[f'molecule_atom_index_0_y_1_mean_diff'] = df[f'molecule_atom_index_0_y_1_mean'] - df['y_1']\n",
    "    df[f'molecule_atom_index_0_y_1_mean_div'] = df[f'molecule_atom_index_0_y_1_mean'] / df['y_1']\n",
    "    df[f'molecule_atom_index_0_y_1_max'] = df.groupby(['molecule_name', 'atom_index_0'])['y_1'].transform('max')\n",
    "    df[f'molecule_atom_index_0_y_1_max_diff'] = df[f'molecule_atom_index_0_y_1_max'] - df['y_1']\n",
    "    df[f'molecule_atom_index_0_y_1_std'] = df.groupby(['molecule_name', 'atom_index_0'])['y_1'].transform('std')\n",
    "    df[f'molecule_atom_index_0_z_1_std'] = df.groupby(['molecule_name', 'atom_index_0'])['z_1'].transform('std')\n",
    "    df[f'molecule_atom_index_0_dist_mean'] = df.groupby(['molecule_name', 'atom_index_0'])['dist'].transform('mean')\n",
    "    df[f'molecule_atom_index_0_dist_mean_diff'] = df[f'molecule_atom_index_0_dist_mean'] - df['dist']\n",
    "    df[f'molecule_atom_index_0_dist_mean_div'] = df[f'molecule_atom_index_0_dist_mean'] / df['dist']\n",
    "    df[f'molecule_atom_index_0_dist_max'] = df.groupby(['molecule_name', 'atom_index_0'])['dist'].transform('max')\n",
    "    df[f'molecule_atom_index_0_dist_max_diff'] = df[f'molecule_atom_index_0_dist_max'] - df['dist']\n",
    "    df[f'molecule_atom_index_0_dist_max_div'] = df[f'molecule_atom_index_0_dist_max'] / df['dist']\n",
    "    df[f'molecule_atom_index_0_dist_min'] = df.groupby(['molecule_name', 'atom_index_0'])['dist'].transform('min')\n",
    "    df[f'molecule_atom_index_0_dist_min_diff'] = df[f'molecule_atom_index_0_dist_min'] - df['dist']\n",
    "    df[f'molecule_atom_index_0_dist_min_div'] = df[f'molecule_atom_index_0_dist_min'] / df['dist']\n",
    "    df[f'molecule_atom_index_0_dist_std'] = df.groupby(['molecule_name', 'atom_index_0'])['dist'].transform('std')\n",
    "    df[f'molecule_atom_index_0_dist_std_diff'] = df[f'molecule_atom_index_0_dist_std'] - df['dist']\n",
    "    df[f'molecule_atom_index_0_dist_std_div'] = df[f'molecule_atom_index_0_dist_std'] / df['dist']\n",
    "    df[f'molecule_atom_index_1_dist_mean'] = df.groupby(['molecule_name', 'atom_index_1'])['dist'].transform('mean')\n",
    "    df[f'molecule_atom_index_1_dist_mean_diff'] = df[f'molecule_atom_index_1_dist_mean'] - df['dist']\n",
    "    df[f'molecule_atom_index_1_dist_mean_div'] = df[f'molecule_atom_index_1_dist_mean'] / df['dist']\n",
    "    df[f'molecule_atom_index_1_dist_max'] = df.groupby(['molecule_name', 'atom_index_1'])['dist'].transform('max')\n",
    "    df[f'molecule_atom_index_1_dist_max_diff'] = df[f'molecule_atom_index_1_dist_max'] - df['dist']\n",
    "    df[f'molecule_atom_index_1_dist_max_div'] = df[f'molecule_atom_index_1_dist_max'] / df['dist']\n",
    "    df[f'molecule_atom_index_1_dist_min'] = df.groupby(['molecule_name', 'atom_index_1'])['dist'].transform('min')\n",
    "    df[f'molecule_atom_index_1_dist_min_diff'] = df[f'molecule_atom_index_1_dist_min'] - df['dist']\n",
    "    df[f'molecule_atom_index_1_dist_min_div'] = df[f'molecule_atom_index_1_dist_min'] / df['dist']\n",
    "    df[f'molecule_atom_index_1_dist_std'] = df.groupby(['molecule_name', 'atom_index_1'])['dist'].transform('std')\n",
    "    df[f'molecule_atom_index_1_dist_std_diff'] = df[f'molecule_atom_index_1_dist_std'] - df['dist']\n",
    "    df[f'molecule_atom_index_1_dist_std_div'] = df[f'molecule_atom_index_1_dist_std'] / df['dist']\n",
    "    df[f'molecule_atom_1_dist_mean'] = df.groupby(['molecule_name', 'atom_1'])['dist'].transform('mean')\n",
    "    df[f'molecule_atom_1_dist_min'] = df.groupby(['molecule_name', 'atom_1'])['dist'].transform('min')\n",
    "    df[f'molecule_atom_1_dist_min_diff'] = df[f'molecule_atom_1_dist_min'] - df['dist']\n",
    "    df[f'molecule_atom_1_dist_min_div'] = df[f'molecule_atom_1_dist_min'] / df['dist']\n",
    "    df[f'molecule_atom_1_dist_std'] = df.groupby(['molecule_name', 'atom_1'])['dist'].transform('std')\n",
    "    df[f'molecule_atom_1_dist_std_diff'] = df[f'molecule_atom_1_dist_std'] - df['dist']\n",
    "    df[f'molecule_type_0_dist_std'] = df.groupby(['molecule_name', 'type_0'])['dist'].transform('std')\n",
    "    df[f'molecule_type_0_dist_std_diff'] = df[f'molecule_type_0_dist_std'] - df['dist']\n",
    "    df[f'molecule_type_dist_mean'] = df.groupby(['molecule_name', 'type'])['dist'].transform('mean')\n",
    "    df[f'molecule_type_dist_mean_diff'] = df[f'molecule_type_dist_mean'] - df['dist']\n",
    "    df[f'molecule_type_dist_mean_div'] = df[f'molecule_type_dist_mean'] / df['dist']\n",
    "    df[f'molecule_type_dist_max'] = df.groupby(['molecule_name', 'type'])['dist'].transform('max')\n",
    "    df[f'molecule_type_dist_min'] = df.groupby(['molecule_name', 'type'])['dist'].transform('min')\n",
    "    df[f'molecule_type_dist_std'] = df.groupby(['molecule_name', 'type'])['dist'].transform('std')\n",
    "    df[f'molecule_type_dist_std_diff'] = df[f'molecule_type_dist_std'] - df['dist']\n",
    "\n",
    "    df = reduce_mem_usage(df)\n",
    "    return df\n",
    "\n",
    "def Feature_enjineering(X):\n",
    "    #データとstructureを統合。\n",
    "    #一つのデータにそれぞれの原子の位置をインプット。\n",
    "    X = map_atom_info(X, 0)\n",
    "    X = map_atom_info(X, 1)\n",
    "    \n",
    "    #原子間の距離計算\n",
    "    #0,1番目の原子の位置\n",
    "    X_p_0 = X[['x_0', 'y_0', 'z_0']].values\n",
    "    X_p_1 = X[['x_1', 'y_1', 'z_1']].values\n",
    "    \n",
    "    #距離(長さ、x,y,z)\n",
    "    X['dist'] = np.linalg.norm(X_p_0 - X_p_1, axis=1)\n",
    "    X['dist_x'] = (X['x_0'] - X['x_1']) ** 2\n",
    "    X['dist_y'] = (X['y_0'] - X['y_1']) ** 2\n",
    "    X['dist_z'] = (X['z_0'] - X['z_1']) ** 2\n",
    "    \n",
    "    #type付加\n",
    "    X['type_0'] = X['type'].apply(lambda x: x[0])\n",
    "    X['type_1'] = X['type'].apply(lambda x: x[1:])\n",
    "    \n",
    "    #それぞれの特徴量ごとに平均をとったdistで正規化した距離。\n",
    "    X['dist_to_type_mean'] = X['dist'] / X.groupby('type')['dist'].transform('mean')\n",
    "    X['dist_to_type_0_mean'] = X['dist'] / X.groupby('type_0')['dist'].transform('mean')\n",
    "    X['dist_to_type_1_mean'] = X['dist'] / X.groupby('type_1')['dist'].transform('mean')\n",
    "    X[f'molecule_type_dist_mean'] = X.groupby(['molecule_name', 'type'])['dist'].transform('mean')\n",
    "    \n",
    "    return X\n",
    "\n",
    "#ラベルエンコード(文字列→数値)\n",
    "def Label_encode(train, test):\n",
    "    for f in ['atom_0', 'atom_1', 'type_0', 'type_1', 'type']:\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(train[f].values) + list(test[f].values))\n",
    "        train[f] = lbl.transform(list(train[f].values))\n",
    "        test[f] = lbl.transform(list(test[f].values))\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>food</th>\n",
       "      <th>price</th>\n",
       "      <th>quantity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>osaka</td>\n",
       "      <td>apple</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>osaka</td>\n",
       "      <td>orange</td>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>osaka</td>\n",
       "      <td>banana</td>\n",
       "      <td>250</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>osaka</td>\n",
       "      <td>banana</td>\n",
       "      <td>300</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tokyo</td>\n",
       "      <td>apple</td>\n",
       "      <td>150</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tokyo</td>\n",
       "      <td>apple</td>\n",
       "      <td>200</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tokyo</td>\n",
       "      <td>banana</td>\n",
       "      <td>400</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    city    food  price  quantity\n",
       "0  osaka   apple    100         1\n",
       "1  osaka  orange    200         2\n",
       "2  osaka  banana    250         3\n",
       "3  osaka  banana    300         4\n",
       "4  tokyo   apple    150         5\n",
       "5  tokyo   apple    200         6\n",
       "6  tokyo  banana    400         7"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'city': ['osaka', 'osaka', 'osaka', 'osaka', 'tokyo', 'tokyo', 'tokyo'],\n",
    "    'food': ['apple', 'orange', 'banana', 'banana', 'apple', 'apple', 'banana'],\n",
    "    'price': [100, 200, 250, 300, 150, 200, 400],\n",
    "    'quantity': [1, 2, 3, 4, 5, 6, 7]\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2.5\n",
       "1    2.5\n",
       "2    2.5\n",
       "3    2.5\n",
       "4    6.0\n",
       "5    6.0\n",
       "6    6.0\n",
       "Name: quantity, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('city')['quantity'].transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#モデル学習\n",
    "\n",
    "#回帰モデル\n",
    "def train_model_regression(X, X_test, y, params, folds, model_type='lgb', eval_metric='mae', columns=None,\n",
    "                           plot_feature_importance=False, model=None,\n",
    "                           verbose=10000, early_stopping_rounds=200, n_estimators=50000):\n",
    "    \"\"\"\n",
    "    A function to train a variety of regression models.\n",
    "    Returns dictionary with oof predictions, test predictions, scores and, if necessary, feature importances.\n",
    "    \n",
    "    :params: X - training data, can be pd.DataFrame or np.ndarray (after normalizing)\n",
    "    :params: X_test - test data, can be pd.DataFrame or np.ndarray (after normalizing)\n",
    "    :params: y - target\n",
    "    :params: folds - folds to split data\n",
    "    :params: model_type - type of model to use\n",
    "    :params: eval_metric - metric to use\n",
    "    :params: columns - columns to use. If None - use all columns\n",
    "    :params: plot_feature_importance - whether to plot feature importance of LGB\n",
    "    :params: model - sklearn model, works only for \"sklearn\" model type\n",
    "    :params: verbose - parameters for gradient boosting models\n",
    "    :params: early_stopping_rounds - parameters for gradient boosting models\n",
    "    :params: n_estimators - parameters for gradient boosting models\n",
    "    \n",
    "    \"\"\"\n",
    "    columns = X.columns if columns is None else columns\n",
    "    X_test = X_test[columns]\n",
    "    \n",
    "    # to set up scoring parameters\n",
    "    metrics_dict = {'mae': {'lgb_metric_name': 'mae',\n",
    "                        'catboost_metric_name': 'MAE',\n",
    "                        'sklearn_scoring_function': metrics.mean_absolute_error},\n",
    "                    'group_mae': {'lgb_metric_name': 'mae',\n",
    "                        'catboost_metric_name': 'MAE',\n",
    "                        'scoring_function': group_mean_log_mae},\n",
    "                    'mse': {'lgb_metric_name': 'mse',\n",
    "                        'catboost_metric_name': 'MSE',\n",
    "                        'sklearn_scoring_function': metrics.mean_squared_error}\n",
    "                    }\n",
    "\n",
    "    \n",
    "    result_dict = {}\n",
    "    \n",
    "    # out-of-fold predictions on train data\n",
    "    oof = np.zeros(len(X))\n",
    "    \n",
    "    # averaged predictions on train data\n",
    "    prediction = np.zeros(len(X_test))\n",
    "    \n",
    "    # list of scores on folds\n",
    "    scores = []\n",
    "    feature_importance = pd.DataFrame()\n",
    "    \n",
    "    # split and train on folds\n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n",
    "        print(f'Fold {fold_n + 1} started at {time.ctime()}')\n",
    "        if type(X) == np.ndarray:\n",
    "            X_train, X_valid = X[columns][train_index], X[columns][valid_index]\n",
    "            y_train, y_valid = y[train_index], y[valid_index]\n",
    "        else:\n",
    "            X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n",
    "            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "            \n",
    "        if model_type == 'lgb':\n",
    "            model = lgb.LGBMRegressor(**params, n_estimators = n_estimators, n_jobs = -1)\n",
    "            model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "                      eval_metric=metrics_dict[eval_metric]['lgb_metric_name'],verbose=verbose,\n",
    "                      early_stopping_rounds=early_stopping_rounds)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n",
    "            \n",
    "        if model_type == 'xgb':\n",
    "            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n",
    "            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n",
    "\n",
    "            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n",
    "            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist,\n",
    "                              early_stopping_rounds=200, verbose_eval=verbose, params=params)\n",
    "            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns),\n",
    "                                         ntree_limit=model.best_ntree_limit)\n",
    "            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns),\n",
    "                                   ntree_limit=model.best_ntree_limit)\n",
    "        \n",
    "        if model_type == 'sklearn':\n",
    "            model = model\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid).reshape(-1,)\n",
    "            score = metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid)\n",
    "            print(f'Fold {fold_n}. {eval_metric}: {score:.4f}.')\n",
    "            print('')\n",
    "            \n",
    "            y_pred = model.predict(X_test).reshape(-1,)\n",
    "        \n",
    "        if model_type == 'cat':\n",
    "            model = CatBoostRegressor(iterations=20000,\n",
    "                                      eval_metric=metrics_dict[eval_metric]['catboost_metric_name'], **params,\n",
    "                                      loss_function=metrics_dict[eval_metric]['catboost_metric_name'])\n",
    "            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[],\n",
    "                      use_best_model=True, verbose=False)\n",
    "\n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test)\n",
    "        \n",
    "        oof[valid_index] = y_pred_valid.reshape(-1,)\n",
    "        if eval_metric != 'group_mae':\n",
    "            scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid))\n",
    "        else:\n",
    "            scores.append(metrics_dict[eval_metric]['scoring_function'](y_valid, y_pred_valid, X_valid['type']))\n",
    "\n",
    "        prediction += y_pred    \n",
    "        \n",
    "        if model_type == 'lgb' and plot_feature_importance:\n",
    "            # feature importance\n",
    "            fold_importance = pd.DataFrame()\n",
    "            fold_importance[\"feature\"] = columns\n",
    "            fold_importance[\"importance\"] = model.feature_importances_\n",
    "            fold_importance[\"fold\"] = fold_n + 1\n",
    "            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n",
    "\n",
    "    prediction /= folds.n_splits\n",
    "    \n",
    "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "    \n",
    "    result_dict['oof'] = oof\n",
    "    result_dict['prediction'] = prediction\n",
    "    result_dict['scores'] = scores\n",
    "    \n",
    "    if model_type == 'lgb':\n",
    "        if plot_feature_importance:\n",
    "            feature_importance[\"importance\"] /= folds.n_splits\n",
    "            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
    "                by=\"importance\", ascending=False)[:50].index\n",
    "\n",
    "            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n",
    "\n",
    "            plt.figure(figsize=(16, 12));\n",
    "            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n",
    "            plt.title('LGB Features (avg over folds)');\n",
    "            \n",
    "            result_dict['feature_importance'] = feature_importance\n",
    "        \n",
    "    return result_dict\n",
    "    \n",
    "\n",
    "#分類モデル\n",
    "def train_model_classification(X, X_test, y, params, folds, model_type='lgb', eval_metric='auc', columns=None, plot_feature_importance=False, model=None,\n",
    "                               verbose=10000, early_stopping_rounds=200, n_estimators=50000):\n",
    "    \"\"\"\n",
    "    A function to train a variety of classification models.\n",
    "    Returns dictionary with oof predictions, test predictions, scores and, if necessary, feature importances.\n",
    "    \n",
    "    :params: X - training data, can be pd.DataFrame or np.ndarray (after normalizing)\n",
    "    :params: X_test - test data, can be pd.DataFrame or np.ndarray (after normalizing)\n",
    "    :params: y - target\n",
    "    :params: folds - folds to split data\n",
    "    :params: model_type - type of model to use\n",
    "    :params: eval_metric - metric to use\n",
    "    :params: columns - columns to use. If None - use all columns\n",
    "    :params: plot_feature_importance - whether to plot feature importance of LGB\n",
    "    :params: model - sklearn model, works only for \"sklearn\" model type\n",
    "    :params: verbose - parameters for gradient boosting models\n",
    "    :params: early_stopping_rounds - parameters for gradient boosting models\n",
    "    :params: n_estimators - parameters for gradient boosting models\n",
    "    \n",
    "    \"\"\"\n",
    "    columns = X.columns if columns == None else columns\n",
    "    X_test = X_test[columns]\n",
    "    \n",
    "    # to set up scoring parameters\n",
    "    metrics_dict = {'auc': {'lgb_metric_name': eval_auc,\n",
    "                        'catboost_metric_name': 'AUC',\n",
    "                        'sklearn_scoring_function': metrics.roc_auc_score},\n",
    "                    }\n",
    "    \n",
    "    result_dict = {}\n",
    "    \n",
    "    # out-of-fold predictions on train data\n",
    "    oof = np.zeros((len(X), len(set(y.values))))\n",
    "    \n",
    "    # averaged predictions on train data\n",
    "    prediction = np.zeros((len(X_test), oof.shape[1]))\n",
    "    \n",
    "    # list of scores on folds\n",
    "    scores = []\n",
    "    feature_importance = pd.DataFrame()\n",
    "    \n",
    "    # split and train on folds\n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n",
    "        print(f'Fold {fold_n + 1} started at {time.ctime()}')\n",
    "        if type(X) == np.ndarray:\n",
    "            X_train, X_valid = X[columns][train_index], X[columns][valid_index]\n",
    "            y_train, y_valid = y[train_index], y[valid_index]\n",
    "        else:\n",
    "            X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n",
    "            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "            \n",
    "        if model_type == 'lgb':\n",
    "            model = lgb.LGBMClassifier(**params, n_estimators=n_estimators, n_jobs = -1)\n",
    "            model.fit(X_train, y_train, \n",
    "                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric=metrics_dict[eval_metric]['lgb_metric_name'],\n",
    "                    verbose=verbose, early_stopping_rounds=early_stopping_rounds)\n",
    "            \n",
    "            y_pred_valid = model.predict_proba(X_valid)\n",
    "            y_pred = model.predict_proba(X_test, num_iteration=model.best_iteration_)\n",
    "            \n",
    "        if model_type == 'xgb':\n",
    "            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n",
    "            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n",
    "\n",
    "            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n",
    "            model = xgb.train(dtrain=train_data, num_boost_round=n_estimators, evals=watchlist, early_stopping_rounds=early_stopping_rounds, verbose_eval=verbose, params=params)\n",
    "            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
    "            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
    "        \n",
    "        if model_type == 'sklearn':\n",
    "            model = model\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid).reshape(-1,)\n",
    "            score = metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid)\n",
    "            print(f'Fold {fold_n}. {eval_metric}: {score:.4f}.')\n",
    "            print('')\n",
    "            \n",
    "            y_pred = model.predict_proba(X_test)\n",
    "        \n",
    "        if model_type == 'cat':\n",
    "            model = CatBoostClassifier(iterations=n_estimators, eval_metric=metrics_dict[eval_metric]['catboost_metric_name'], **params,\n",
    "                                      loss_function=metrics_dict[eval_metric]['catboost_metric_name'])\n",
    "            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n",
    "\n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test)\n",
    "        \n",
    "        oof[valid_index] = y_pred_valid\n",
    "        scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid[:, 1]))\n",
    "\n",
    "        prediction += y_pred    \n",
    "        \n",
    "        if model_type == 'lgb' and plot_feature_importance:\n",
    "            # feature importance\n",
    "            fold_importance = pd.DataFrame()\n",
    "            fold_importance[\"feature\"] = columns\n",
    "            fold_importance[\"importance\"] = model.feature_importances_\n",
    "            fold_importance[\"fold\"] = fold_n + 1\n",
    "            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n",
    "\n",
    "    prediction /= folds.n_splits\n",
    "    \n",
    "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "    \n",
    "    result_dict['oof'] = oof\n",
    "    result_dict['prediction'] = prediction\n",
    "    result_dict['scores'] = scores\n",
    "    \n",
    "    if model_type == 'lgb':\n",
    "        if plot_feature_importance:\n",
    "            feature_importance[\"importance\"] /= folds.n_splits\n",
    "            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
    "                by=\"importance\", ascending=False)[:50].index\n",
    "\n",
    "            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n",
    "\n",
    "            plt.figure(figsize=(16, 12));\n",
    "            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n",
    "            plt.title('LGB Features (avg over folds)');\n",
    "            \n",
    "            result_dict['feature_importance'] = feature_importance\n",
    "        \n",
    "    return result_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#実行：特徴量エンジニアリング：feature_enjineering\n",
    "trainFeature = Feature_enjineering(train)\n",
    "testFeature  = Feature_enjineering(test)\n",
    "\n",
    "trainFeature, testFeature = Label_encode(trainFeature, testFeature)\n",
    "\n",
    "#特徴量と教師の生成\n",
    "X = trainFeature.drop(['id', 'molecule_name', 'scalar_coupling_constant'], axis=1)\n",
    "y = trainFeature['scalar_coupling_constant']\n",
    "X_test = testFeature.drop(['id', 'molecule_name'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>atom_index_0</th>\n",
       "      <th>atom_index_1</th>\n",
       "      <th>type</th>\n",
       "      <th>atom_0</th>\n",
       "      <th>x_0</th>\n",
       "      <th>y_0</th>\n",
       "      <th>z_0</th>\n",
       "      <th>atom_1</th>\n",
       "      <th>x_1</th>\n",
       "      <th>y_1</th>\n",
       "      <th>...</th>\n",
       "      <th>dist</th>\n",
       "      <th>dist_x</th>\n",
       "      <th>dist_y</th>\n",
       "      <th>dist_z</th>\n",
       "      <th>type_0</th>\n",
       "      <th>type_1</th>\n",
       "      <th>dist_to_type_mean</th>\n",
       "      <th>dist_to_type_0_mean</th>\n",
       "      <th>dist_to_type_1_mean</th>\n",
       "      <th>molecule_type_dist_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0021504160</td>\n",
       "      <td>-0.0060313176</td>\n",
       "      <td>0.0019761204</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0126981359</td>\n",
       "      <td>1.085804158</td>\n",
       "      <td>...</td>\n",
       "      <td>1.091953059611900</td>\n",
       "      <td>0.000220479493527</td>\n",
       "      <td>1.192104705778678</td>\n",
       "      <td>0.000036299123586</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999133993514757</td>\n",
       "      <td>1.003366595134415</td>\n",
       "      <td>0.463061408177816</td>\n",
       "      <td>1.091949649609598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0021504160</td>\n",
       "      <td>-0.0060313176</td>\n",
       "      <td>0.0019761204</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0117308430</td>\n",
       "      <td>1.463751162</td>\n",
       "      <td>...</td>\n",
       "      <td>1.783119756038801</td>\n",
       "      <td>1.019252638581502</td>\n",
       "      <td>2.160260537339124</td>\n",
       "      <td>0.000002888455246</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.004633785679569</td>\n",
       "      <td>0.852949262177106</td>\n",
       "      <td>0.761935207514872</td>\n",
       "      <td>1.783146310703152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0021504160</td>\n",
       "      <td>-0.0060313176</td>\n",
       "      <td>0.0019761204</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.5408150690</td>\n",
       "      <td>1.447526614</td>\n",
       "      <td>...</td>\n",
       "      <td>1.783147496403011</td>\n",
       "      <td>0.294811517901285</td>\n",
       "      <td>2.112830660517270</td>\n",
       "      <td>0.771972815509771</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.004649414975929</td>\n",
       "      <td>0.852962531685845</td>\n",
       "      <td>0.761947061099068</td>\n",
       "      <td>1.783146310703152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0021504160</td>\n",
       "      <td>-0.0060313176</td>\n",
       "      <td>0.0019761204</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.5238136345</td>\n",
       "      <td>1.437932644</td>\n",
       "      <td>...</td>\n",
       "      <td>1.783156685329616</td>\n",
       "      <td>0.276638182418367</td>\n",
       "      <td>2.085031922399566</td>\n",
       "      <td>0.817977659617770</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.004654592141451</td>\n",
       "      <td>0.852966927177590</td>\n",
       "      <td>0.761950987569333</td>\n",
       "      <td>1.783146310703152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0117308430</td>\n",
       "      <td>1.4637511620</td>\n",
       "      <td>0.0002765748</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0126981359</td>\n",
       "      <td>1.085804158</td>\n",
       "      <td>...</td>\n",
       "      <td>1.091951618581363</td>\n",
       "      <td>1.049454732810097</td>\n",
       "      <td>0.142843937832576</td>\n",
       "      <td>0.000059666679785</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999132674975848</td>\n",
       "      <td>1.003365271009819</td>\n",
       "      <td>0.463060797084121</td>\n",
       "      <td>1.091949649609598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0117308430</td>\n",
       "      <td>1.4637511620</td>\n",
       "      <td>0.0002765748</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.5408150690</td>\n",
       "      <td>1.447526614</td>\n",
       "      <td>...</td>\n",
       "      <td>1.783157659838131</td>\n",
       "      <td>2.410398808867912</td>\n",
       "      <td>0.000263235957804</td>\n",
       "      <td>0.768989195013684</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.004655141192728</td>\n",
       "      <td>0.852967393330420</td>\n",
       "      <td>0.761951403981267</td>\n",
       "      <td>1.783146310703152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0117308430</td>\n",
       "      <td>1.4637511620</td>\n",
       "      <td>0.0002765748</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.5238136345</td>\n",
       "      <td>1.437932644</td>\n",
       "      <td>...</td>\n",
       "      <td>1.783148394379570</td>\n",
       "      <td>2.357896842380748</td>\n",
       "      <td>0.000666595871716</td>\n",
       "      <td>0.821054758125974</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.004649920908067</td>\n",
       "      <td>0.852962961229874</td>\n",
       "      <td>0.761947444808549</td>\n",
       "      <td>1.783146310703152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.5408150690</td>\n",
       "      <td>1.4475266140</td>\n",
       "      <td>-0.8766437152</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0126981359</td>\n",
       "      <td>1.085804158</td>\n",
       "      <td>...</td>\n",
       "      <td>1.091946379133103</td>\n",
       "      <td>0.278907495026950</td>\n",
       "      <td>0.130843135174672</td>\n",
       "      <td>0.782596264700273</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999127880895354</td>\n",
       "      <td>1.003360456620304</td>\n",
       "      <td>0.463058575206296</td>\n",
       "      <td>1.091949649609598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.5408150690</td>\n",
       "      <td>1.4475266140</td>\n",
       "      <td>-0.8766437152</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.5238136345</td>\n",
       "      <td>1.437932644</td>\n",
       "      <td>...</td>\n",
       "      <td>1.783147872229780</td>\n",
       "      <td>0.000289048775058</td>\n",
       "      <td>0.000092044260361</td>\n",
       "      <td>3.179235241202171</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.004649626721813</td>\n",
       "      <td>0.852962711461300</td>\n",
       "      <td>0.761947221691561</td>\n",
       "      <td>1.783146310703152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.5238136345</td>\n",
       "      <td>1.4379326440</td>\n",
       "      <td>0.9063972942</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.0126981359</td>\n",
       "      <td>1.085804158</td>\n",
       "      <td>...</td>\n",
       "      <td>1.091947541112027</td>\n",
       "      <td>0.261239052909127</td>\n",
       "      <td>0.123994470652652</td>\n",
       "      <td>0.807115908978822</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999128944102817</td>\n",
       "      <td>1.003361524331801</td>\n",
       "      <td>0.463059067963372</td>\n",
       "      <td>1.091949649609598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   atom_index_0  atom_index_1  type  atom_0           x_0           y_0  \\\n",
       "0             1             0     0       0  0.0021504160 -0.0060313176   \n",
       "1             1             2     3       0  0.0021504160 -0.0060313176   \n",
       "2             1             3     3       0  0.0021504160 -0.0060313176   \n",
       "3             1             4     3       0  0.0021504160 -0.0060313176   \n",
       "4             2             0     0       0  1.0117308430  1.4637511620   \n",
       "5             2             3     3       0  1.0117308430  1.4637511620   \n",
       "6             2             4     3       0  1.0117308430  1.4637511620   \n",
       "7             3             0     0       0 -0.5408150690  1.4475266140   \n",
       "8             3             4     3       0 -0.5408150690  1.4475266140   \n",
       "9             4             0     0       0 -0.5238136345  1.4379326440   \n",
       "\n",
       "            z_0  atom_1           x_1          y_1           ...             \\\n",
       "0  0.0019761204       0 -0.0126981359  1.085804158           ...              \n",
       "1  0.0019761204       1  1.0117308430  1.463751162           ...              \n",
       "2  0.0019761204       1 -0.5408150690  1.447526614           ...              \n",
       "3  0.0019761204       1 -0.5238136345  1.437932644           ...              \n",
       "4  0.0002765748       0 -0.0126981359  1.085804158           ...              \n",
       "5  0.0002765748       1 -0.5408150690  1.447526614           ...              \n",
       "6  0.0002765748       1 -0.5238136345  1.437932644           ...              \n",
       "7 -0.8766437152       0 -0.0126981359  1.085804158           ...              \n",
       "8 -0.8766437152       1 -0.5238136345  1.437932644           ...              \n",
       "9  0.9063972942       0 -0.0126981359  1.085804158           ...              \n",
       "\n",
       "                dist             dist_x             dist_y             dist_z  \\\n",
       "0  1.091953059611900  0.000220479493527  1.192104705778678  0.000036299123586   \n",
       "1  1.783119756038801  1.019252638581502  2.160260537339124  0.000002888455246   \n",
       "2  1.783147496403011  0.294811517901285  2.112830660517270  0.771972815509771   \n",
       "3  1.783156685329616  0.276638182418367  2.085031922399566  0.817977659617770   \n",
       "4  1.091951618581363  1.049454732810097  0.142843937832576  0.000059666679785   \n",
       "5  1.783157659838131  2.410398808867912  0.000263235957804  0.768989195013684   \n",
       "6  1.783148394379570  2.357896842380748  0.000666595871716  0.821054758125974   \n",
       "7  1.091946379133103  0.278907495026950  0.130843135174672  0.782596264700273   \n",
       "8  1.783147872229780  0.000289048775058  0.000092044260361  3.179235241202171   \n",
       "9  1.091947541112027  0.261239052909127  0.123994470652652  0.807115908978822   \n",
       "\n",
       "   type_0  type_1  dist_to_type_mean  dist_to_type_0_mean  \\\n",
       "0       0       0  0.999133993514757    1.003366595134415   \n",
       "1       1       1  1.004633785679569    0.852949262177106   \n",
       "2       1       1  1.004649414975929    0.852962531685845   \n",
       "3       1       1  1.004654592141451    0.852966927177590   \n",
       "4       0       0  0.999132674975848    1.003365271009819   \n",
       "5       1       1  1.004655141192728    0.852967393330420   \n",
       "6       1       1  1.004649920908067    0.852962961229874   \n",
       "7       0       0  0.999127880895354    1.003360456620304   \n",
       "8       1       1  1.004649626721813    0.852962711461300   \n",
       "9       0       0  0.999128944102817    1.003361524331801   \n",
       "\n",
       "   dist_to_type_1_mean  molecule_type_dist_mean  \n",
       "0    0.463061408177816        1.091949649609598  \n",
       "1    0.761935207514872        1.783146310703152  \n",
       "2    0.761947061099068        1.783146310703152  \n",
       "3    0.761950987569333        1.783146310703152  \n",
       "4    0.463060797084121        1.091949649609598  \n",
       "5    0.761951403981267        1.783146310703152  \n",
       "6    0.761947444808549        1.783146310703152  \n",
       "7    0.463058575206296        1.091949649609598  \n",
       "8    0.761947221691561        1.783146310703152  \n",
       "9    0.463059067963372        1.091949649609598  \n",
       "\n",
       "[10 rows x 21 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 started at Wed Jul 17 05:42:57 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[1000]\ttraining's l1: 1.76127\tvalid_1's l1: 1.87648\n",
      "[2000]\ttraining's l1: 1.60585\tvalid_1's l1: 1.80265\n",
      "[3000]\ttraining's l1: 1.4983\tvalid_1's l1: 1.76282\n",
      "[4000]\ttraining's l1: 1.41287\tvalid_1's l1: 1.73744\n",
      "[5000]\ttraining's l1: 1.34115\tvalid_1's l1: 1.71869\n",
      "[6000]\ttraining's l1: 1.27835\tvalid_1's l1: 1.70459\n",
      "[7000]\ttraining's l1: 1.22187\tvalid_1's l1: 1.69249\n",
      "[8000]\ttraining's l1: 1.17112\tvalid_1's l1: 1.68241\n",
      "[9000]\ttraining's l1: 1.12396\tvalid_1's l1: 1.67365\n",
      "[10000]\ttraining's l1: 1.08133\tvalid_1's l1: 1.66681\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's l1: 1.08133\tvalid_1's l1: 1.66681\n",
      "Fold 2 started at Wed Jul 17 06:49:38 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[1000]\ttraining's l1: 1.76014\tvalid_1's l1: 1.87743\n",
      "[2000]\ttraining's l1: 1.6056\tvalid_1's l1: 1.80467\n",
      "[3000]\ttraining's l1: 1.49884\tvalid_1's l1: 1.7664\n",
      "[4000]\ttraining's l1: 1.41283\tvalid_1's l1: 1.73966\n",
      "[5000]\ttraining's l1: 1.34085\tvalid_1's l1: 1.72081\n",
      "[6000]\ttraining's l1: 1.2774\tvalid_1's l1: 1.70593\n",
      "[7000]\ttraining's l1: 1.2211\tvalid_1's l1: 1.69387\n",
      "[8000]\ttraining's l1: 1.16995\tvalid_1's l1: 1.68376\n",
      "[9000]\ttraining's l1: 1.12345\tvalid_1's l1: 1.67538\n",
      "[10000]\ttraining's l1: 1.08062\tvalid_1's l1: 1.66828\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's l1: 1.08062\tvalid_1's l1: 1.66828\n",
      "Fold 3 started at Wed Jul 17 07:55:44 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[1000]\ttraining's l1: 1.75982\tvalid_1's l1: 1.87885\n",
      "[2000]\ttraining's l1: 1.60648\tvalid_1's l1: 1.8069\n",
      "[3000]\ttraining's l1: 1.50019\tvalid_1's l1: 1.76924\n",
      "[4000]\ttraining's l1: 1.41421\tvalid_1's l1: 1.74212\n",
      "[5000]\ttraining's l1: 1.34162\tvalid_1's l1: 1.72201\n",
      "[6000]\ttraining's l1: 1.27847\tvalid_1's l1: 1.70621\n",
      "[7000]\ttraining's l1: 1.2219\tvalid_1's l1: 1.69353\n",
      "[8000]\ttraining's l1: 1.1709\tvalid_1's l1: 1.68337\n",
      "[9000]\ttraining's l1: 1.12402\tvalid_1's l1: 1.67476\n",
      "[10000]\ttraining's l1: 1.08174\tvalid_1's l1: 1.66769\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's l1: 1.08174\tvalid_1's l1: 1.66769\n",
      "Fold 4 started at Wed Jul 17 09:01:02 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[1000]\ttraining's l1: 1.76111\tvalid_1's l1: 1.87731\n",
      "[2000]\ttraining's l1: 1.60662\tvalid_1's l1: 1.80426\n",
      "[3000]\ttraining's l1: 1.49934\tvalid_1's l1: 1.76483\n",
      "[4000]\ttraining's l1: 1.41319\tvalid_1's l1: 1.73786\n",
      "[5000]\ttraining's l1: 1.34119\tvalid_1's l1: 1.71823\n",
      "[6000]\ttraining's l1: 1.27839\tvalid_1's l1: 1.70352\n",
      "[7000]\ttraining's l1: 1.22205\tvalid_1's l1: 1.6915\n",
      "[8000]\ttraining's l1: 1.17126\tvalid_1's l1: 1.68198\n",
      "[9000]\ttraining's l1: 1.12451\tvalid_1's l1: 1.67319\n",
      "[10000]\ttraining's l1: 1.08165\tvalid_1's l1: 1.66589\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's l1: 1.08165\tvalid_1's l1: 1.66589\n",
      "Fold 5 started at Wed Jul 17 10:06:54 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[1000]\ttraining's l1: 1.75951\tvalid_1's l1: 1.87898\n",
      "[2000]\ttraining's l1: 1.60622\tvalid_1's l1: 1.80789\n",
      "[3000]\ttraining's l1: 1.49965\tvalid_1's l1: 1.7697\n",
      "[4000]\ttraining's l1: 1.41421\tvalid_1's l1: 1.74392\n",
      "[5000]\ttraining's l1: 1.34128\tvalid_1's l1: 1.72445\n",
      "[6000]\ttraining's l1: 1.27811\tvalid_1's l1: 1.70947\n",
      "[7000]\ttraining's l1: 1.22148\tvalid_1's l1: 1.69754\n",
      "[8000]\ttraining's l1: 1.17063\tvalid_1's l1: 1.68811\n",
      "[9000]\ttraining's l1: 1.12348\tvalid_1's l1: 1.67923\n",
      "[10000]\ttraining's l1: 1.08046\tvalid_1's l1: 1.672\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's l1: 1.08046\tvalid_1's l1: 1.672\n",
      "CV mean score: 0.3646, std: 0.0026.\n",
      "Wall time: 5h 29min 55s\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x864 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "#学習、評価：feature_enjineering\n",
    "\n",
    "#化学コンペ用のscore計算。\n",
    "def group_mean_log_mae(y_true, y_pred, types, floor=1e-9):\n",
    "    \"\"\"\n",
    "    Fast metric computation for this competition: https://www.kaggle.com/c/champs-scalar-coupling\n",
    "    Code is from this kernel: https://www.kaggle.com/uberkinder/efficient-metric\n",
    "    \"\"\"\n",
    "    maes = (y_true-y_pred).abs().groupby(types).mean()\n",
    "    return np.log(maes.map(lambda x: max(x, floor))).mean()\n",
    "\n",
    "#交差検証\n",
    "n_fold = 5\n",
    "folds = KFold(n_splits=n_fold, shuffle=True, random_state=11)\n",
    "\n",
    "params = {'num_leaves': 128,\n",
    "          'min_child_samples': 79,\n",
    "          'objective': 'regression',\n",
    "          'max_depth': 13,\n",
    "          'learning_rate': 0.2,\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"subsample_freq\": 1,\n",
    "          \"subsample\": 0.9,\n",
    "          \"bagging_seed\": 11,\n",
    "          \"metric\": 'mae',\n",
    "          \"verbosity\": -1,\n",
    "          'reg_alpha': 0.1,\n",
    "          'reg_lambda': 0.3,\n",
    "          'colsample_bytree': 1.0\n",
    "         }\n",
    "#lgb(決定木)で学習実行\n",
    "result_dict_lgb = train_model_regression(X=X, X_test=X_test, y=y, params=params, folds=folds, model_type='lgb', eval_metric='group_mae',\n",
    "                                         plot_feature_importance=True, verbose=1000, early_stopping_rounds=200, n_estimators=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>scalar_coupling_constant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4658147</td>\n",
       "      <td>5.897890024118444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4658148</td>\n",
       "      <td>196.424409821197571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4658149</td>\n",
       "      <td>2.478845789730463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4658150</td>\n",
       "      <td>194.323967817800025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4658151</td>\n",
       "      <td>5.396669522683487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  scalar_coupling_constant\n",
       "0  4658147         5.897890024118444\n",
       "1  4658148       196.424409821197571\n",
       "2  4658149         2.478845789730463\n",
       "3  4658150       194.323967817800025\n",
       "4  4658151         5.396669522683487"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#実行：特徴量エンジニアリング：feature_enjineering\n",
    "sub_feature_enjineering = pd.read_csv('sample_submission.csv')\n",
    "sub['scalar_coupling_constant'] = result_dict_lgb['prediction']\n",
    "sub.to_csv('submission_feature_enjineering.csv', index=False)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
